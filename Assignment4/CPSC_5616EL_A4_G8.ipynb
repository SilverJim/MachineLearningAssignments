{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7934c1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the required machine learning libraries and models\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score, classification_report, confusion_matrix\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02dfbbb5",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from data path: ./data\n"
     ]
    }
   ],
   "source": [
    "# All the data should put into the data folder\n",
    "data_filename = 'data'\n",
    "\n",
    "if sys.modules.get(\"google.colab\") is None:\n",
    "    data_path_prefix = \".\"\n",
    "else:\n",
    "    from google.colab import drive\n",
    "    drive.mount(\"/content/drive\")\n",
    "    data_path_prefix = \"/content/drive/MyDrive/MachineLearningAssignments/Assignment4\"\n",
    "\n",
    "data_path = f\"{data_path_prefix}/{data_filename}\"\n",
    "\n",
    "print(f\"Loading data from data path: {data_path}\")\n",
    "\n",
    "# Define the exercise types\n",
    "data_sets = [\n",
    "   {\"path\": \"Jumping_Jack_x10\", \"name\": \"Jumping_Jack\"}, \n",
    "   {\"path\": \"Lunges_x10\", \"name\": \"Lunges\"}, \n",
    "   {\"path\": \"Squat_x10\", \"name\": \"Squat\"}, \n",
    "   {\"path\": \"Jumping_Jack_x10_new\", \"name\": \"Jumping_Jack_new\"}, \n",
    "   {\"path\": \"Lunges_x10_new\", \"name\": \"Lunges_new\"}, \n",
    "   {\"path\": \"Squat_x10\", \"name\": \"Squat_new\"}, \n",
    "]\n",
    "\n",
    "# Define the data sets\n",
    "data_set = [(\"Accelerometer.csv\", \"Accelerometer\"), (\"TotalAcceleration.csv\", \"TotalAcceleration\"), (\"Orientation.csv\", \"Orientation\")]\n",
    "\n",
    "df_list = []\n",
    "\n",
    "# In all the data set after \"time\" is removed, only \"seconds_elapsed\" and \"data_set\" are non-value columns\n",
    "non_value_columns = [\"seconds_elapsed\", \"data_set\"]\n",
    "non_value_columns_set = set(non_value_columns)\n",
    "# read every dataset in every exercise type\n",
    "for file_name, data_set_name in data_set:\n",
    "    temp_df_list = []\n",
    "    for data_set in data_sets:\n",
    "        df = pd.read_csv(f\"{data_path}/{data_set['path']}/{file_name}\")\n",
    "        # There is \"seconds_elapsed\" so \"time\" is not necessary\n",
    "        df.drop(\"time\", axis=1, inplace=True)\n",
    "        # add the data_set column according to which exercise the data from\n",
    "        df['data_set'] = data_set['name']\n",
    "        # Rename the columns according to which dataset the data from to avoid same column\n",
    "        for column in df.columns:\n",
    "            if column not in non_value_columns_set:\n",
    "                df.rename(columns={column: f\"{data_set_name}_{column}\"}, inplace=True)\n",
    "        value_columns = list(set(df.columns) - non_value_columns_set)\n",
    "        # Noise Reduction: calculate rolling mean for all the value columns\n",
    "        df[value_columns] = df[value_columns].rolling(50).mean()\n",
    "        # rolling mean create some NaN values, so we should drop them\n",
    "        df.dropna(inplace=True)\n",
    "        # remove the data at beginning and end of each exercise, as the excercise has not started or has ended\n",
    "        df = df[(df[\"seconds_elapsed\"].quantile(0.1) < df[\"seconds_elapsed\"]) & (df['seconds_elapsed'] < df['seconds_elapsed'].quantile(0.9))]\n",
    "        temp_df_list.append(df)\n",
    "    # combine dataframe from all the exercise types together and put it in to the df_list\n",
    "    df_list.append(pd.concat(temp_df_list))\n",
    "    \n",
    "# Normalization\n",
    "for i in range(len(df_list)):\n",
    "    df = df_list[i]\n",
    "    value_columns = list(set(df.columns) - non_value_columns_set)\n",
    "    scaler = ColumnTransformer(\n",
    "        [\n",
    "            ('standard_scaler', StandardScaler(), value_columns),\n",
    "            ('other', 'passthrough', non_value_columns)\n",
    "\n",
    "        ],\n",
    "    )\n",
    "    scaled_data = scaler.fit_transform(df)\n",
    "    df_list[i] = pd.DataFrame(scaled_data, columns=value_columns + non_value_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85bf6d0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Jumping_Jack': 4.1496669921875, 'Lunges': 5.907152587890625, 'Squat': 5.70484130859375, 'Jumping_Jack_new': 4.027690185546875, 'Lunges_new': 6.39675537109375, 'Squat_new': 5.70484130859375} {'Jumping_Jack': 3.1924683352539063, 'Lunges': 4.997698285205079, 'Squat': 4.79562354765625, 'Jumping_Jack_new': 3.1396086245605472, 'Lunges_new': 5.482244061572265, 'Squat_new': 4.79562354765625}\n",
      "{'Jumping_Jack': 4.1496669921875, 'Lunges': 5.907152587890625, 'Squat': 5.70484130859375, 'Jumping_Jack_new': 4.027690185546875, 'Lunges_new': 6.39675537109375, 'Squat_new': 5.70484130859375} {'Jumping_Jack': 2.8377496313368056, 'Lunges': 4.442398475737848, 'Squat': 4.2627764868055555, 'Jumping_Jack_new': 2.7907632218315976, 'Lunges_new': 4.8731058325086805, 'Squat_new': 4.2627764868055555}\n",
      "{'Jumping_Jack': 4.1496669921875, 'Lunges': 5.907152587890625, 'Squat': 5.70484130859375, 'Jumping_Jack_new': 4.027690185546875, 'Lunges_new': 6.39675537109375, 'Squat_new': 5.70484130859375} {'Jumping_Jack': 2.553974668203125, 'Lunges': 3.998158628164063, 'Squat': 3.8364988381250003, 'Jumping_Jack_new': 2.5116868996484376, 'Lunges_new': 4.3857952492578125, 'Squat_new': 3.8364988381250003}\n",
      "{'Jumping_Jack': 4.1496669921875, 'Lunges': 5.907152587890625, 'Squat': 5.70484130859375, 'Jumping_Jack_new': 4.027690185546875, 'Lunges_new': 6.39675537109375, 'Squat_new': 5.70484130859375} {'Jumping_Jack': 2.321795152911932, 'Lunges': 3.63468966196733, 'Squat': 3.487726216477273, 'Jumping_Jack_new': 2.2833517269531254, 'Lunges_new': 3.987086590234375, 'Squat_new': 3.487726216477273}\n",
      "{'Jumping_Jack': 4.1496669921875, 'Lunges': 5.907152587890625, 'Squat': 5.70484130859375, 'Jumping_Jack_new': 4.027690185546875, 'Lunges_new': 6.39675537109375, 'Squat_new': 5.70484130859375} {'Jumping_Jack': 2.128312223502604, 'Lunges': 3.331798856803386, 'Squat': 3.197082365104167, 'Jumping_Jack_new': 2.093072416373698, 'Lunges_new': 3.65482937438151, 'Squat_new': 3.197082365104167}\n"
     ]
    }
   ],
   "source": [
    "# Window by dividing the data of each exercise into eaqual parts to apply data augmentation we use several different window sizes\n",
    "\n",
    "# If the length of the array is less than the target length, pad it with zeros and make the original array at the beginning\n",
    "def pad(original_series, target_length):\n",
    "    new_series = original_series.copy()\n",
    "    for i in range(original_series.size):\n",
    "        line = original_series[i]\n",
    "        original_length = len(line)\n",
    "        if original_length > target_length:\n",
    "            new_line = line[:target_length]\n",
    "        else:\n",
    "            new_line = np.pad(line, ((0, target_length - original_length), (0, 0)), mode=\"constant\")\n",
    "        new_series[i] = new_line\n",
    "    return new_series\n",
    "\n",
    "windows_count_list = [8, 9, 10, 11, 12]\n",
    "df_concat_list = []\n",
    "for windows_count in windows_count_list:\n",
    "    seconds_elapsed_min = {}\n",
    "    windows_size = {}\n",
    "    for item in data_sets:\n",
    "        data_set = item[\"name\"]\n",
    "        seconds_elapsed_min[data_set] = min((df[df[\"data_set\"] == data_set][\"seconds_elapsed\"].min() for df in df_list))\n",
    "        seconds_elapsed_max = max((df[df[\"data_set\"] == data_set][\"seconds_elapsed\"].max() for df in df_list)) + 0.0000001\n",
    "        windows_size[data_set] = (seconds_elapsed_max - seconds_elapsed_min[data_set]) / windows_count\n",
    "    print(seconds_elapsed_min, windows_size)\n",
    "\n",
    "    # process window and calculate min, max, mean, std value of each window\n",
    "    count_after_padding = 200\n",
    "    df_list_temp = []\n",
    "    for i, df in enumerate(df_list):\n",
    "        df = df.copy()\n",
    "        df[\"window\"] = df[[\"seconds_elapsed\", \"data_set\"]].apply(lambda row: math.floor((row[\"seconds_elapsed\"] - seconds_elapsed_min[row[\"data_set\"]]) / windows_size[row[\"data_set\"]]), axis=1)\n",
    "        df = df.sort_values([\"data_set\", \"window\", \"seconds_elapsed\"])\n",
    "        df.drop(\"seconds_elapsed\", axis=1, inplace=True)\n",
    "        df = (\n",
    "            df\n",
    "            .groupby([\"data_set\", \"window\"], group_keys=True)\n",
    "            .apply(lambda x: np.array(x[[column for column in df.columns if column not in {\"data_set\", \"window\"}]]))\n",
    "        ).to_frame()\n",
    "        df[0] = df.apply(lambda x: pad(x, count_after_padding))\n",
    "        df.rename(columns={0: i}, inplace=True)\n",
    "        df_list_temp.append(df)\n",
    "    # join the dataframe from all the dataset together according to index \"windows\" and \"data_set\"\n",
    "    df_concat_temp = pd.concat(df_list_temp, axis=1)\n",
    "    df_concat_temp[\"windows_count\"] = windows_count\n",
    "    df_concat_list.append(df_concat_temp)\n",
    "df_concat = pd.concat(df_concat_list)\n",
    "df_concat.reset_index(inplace=True)\n",
    "# Make the data_set the last column\n",
    "data_set_mappping = {\n",
    "    \"Jumping_Jack\": 0,\n",
    "    \"Lunges\": 1,\n",
    "    \"Squat\": 2,\n",
    "    \"Jumping_Jack_new\": 0,\n",
    "    \"Lunges_new\": 1,\n",
    "    \"Squat_new\": 2,\n",
    "}\n",
    "df_concat[\"exercise_type\"] = df_concat[\"data_set\"].map(data_set_mappping)\n",
    "df_concat.drop([\"data_set\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a9b19fa",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     window  windows_count                                           features  \\\n",
      "0         0              8  [[-1.5695887046081776, 0.2148409640207338, 0.3...   \n",
      "1         1              8  [[0.8507342366849948, 0.9566529490333389, -1.1...   \n",
      "2         2              8  [[-2.5598215168907585, 1.7755334870284505, 1.3...   \n",
      "3         3              8  [[2.930671137397125, 1.309237396359241, -1.125...   \n",
      "4         4              8  [[-1.2542389201316566, 0.12381020968409308, 0....   \n",
      "..      ...            ...                                                ...   \n",
      "295       7             12  [[0.3267645829981819, -0.21426879701074242, 0....   \n",
      "296       8             12  [[1.1940232312300132, 0.4111327570286471, 0.22...   \n",
      "297       9             12  [[0.7831527569642023, -0.6273576309412399, 0.1...   \n",
      "298      10             12  [[-0.01672499854054074, -0.5163033866401274, 0...   \n",
      "299      11             12  [[-0.7151808472320134, 0.24898119060516596, 0....   \n",
      "\n",
      "     exercise_type  \n",
      "0                0  \n",
      "1                0  \n",
      "2                0  \n",
      "3                0  \n",
      "4                0  \n",
      "..             ...  \n",
      "295              2  \n",
      "296              2  \n",
      "297              2  \n",
      "298              2  \n",
      "299              2  \n",
      "\n",
      "[300 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "df_concat[\"features\"] = df_concat.apply(lambda x:np.concatenate([x[0], x[1], x[2]], axis=1), axis=1)\n",
    "df_concat.drop([0, 1, 2], axis=1, inplace=True)\n",
    "temp = df_concat.pop(\"exercise_type\")\n",
    "df_concat[\"exercise_type\"] = temp\n",
    "print(df_concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02c6aca3",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              features  exercise_type\n",
      "0    [[-1.5695887046081776, 0.2148409640207338, 0.3...              0\n",
      "1    [[0.8507342366849948, 0.9566529490333389, -1.1...              0\n",
      "2    [[-2.5598215168907585, 1.7755334870284505, 1.3...              0\n",
      "3    [[2.930671137397125, 1.309237396359241, -1.125...              0\n",
      "4    [[-1.2542389201316566, 0.12381020968409308, 0....              0\n",
      "..                                                 ...            ...\n",
      "292  [[-0.48708095569365356, 0.30483245660976754, 0...              2\n",
      "293  [[-0.5416458186250555, 0.35358906022672437, 0....              2\n",
      "294  [[-0.8749470578231893, -0.3314822076043692, 0....              2\n",
      "295  [[0.3267645829981819, -0.21426879701074242, 0....              2\n",
      "296  [[1.1940232312300132, 0.4111327570286471, 0.22...              2\n",
      "\n",
      "[222 rows x 2 columns]\n",
      "                                              features  exercise_type\n",
      "7    [[0.7894104613625587, -0.7266671600498076, -0....              0\n",
      "15   [[1.1008682937670211, -0.08986833758746772, 0....              0\n",
      "23   [[-0.44868672162222817, -0.4849840587419749, 0...              1\n",
      "31   [[-0.5414740784456678, -1.489188127084881, 0.6...              1\n",
      "39   [[-0.1977959222111066, -0.6116867500803779, -0...              2\n",
      "47   [[-0.1977959222111066, -0.6116867500803779, -0...              2\n",
      "56   [[-1.0157741562911824, 0.17418894750361627, 0....              0\n",
      "65   [[-1.0009313999224645, -0.18874751732615794, 1...              0\n",
      "74   [[0.7160646248035505, 1.5196966449998783, -1.7...              1\n",
      "83   [[-0.07851421526578999, -0.594052200441339, 0....              1\n",
      "92   [[-1.0543844711662511, -0.0389568100502965, 0....              2\n",
      "101  [[-1.0543844711662511, -0.0389568100502965, 0....              2\n",
      "110  [[-2.5527870460628135, 1.154669783839552, 0.70...              0\n",
      "111  [[1.579484884350927, 0.8376978705808282, -1.23...              0\n",
      "120  [[0.8649881958396037, -0.1502559552656285, 0.2...              0\n",
      "121  [[2.344917608186814, 0.41793621896601024, -0.6...              0\n",
      "130  [[1.0453044747380378, 2.1551554939690996, 1.51...              1\n",
      "131  [[1.0452960845101857, 2.2540566214427553, -2.1...              1\n",
      "140  [[-0.19113669783656398, -0.5705216089020205, -...              1\n",
      "141  [[-0.043869180459292535, -0.4741517824648357, ...              1\n",
      "150  [[-0.9162021541613891, 0.20073783465193695, 0....              2\n",
      "151  [[-0.671387869150753, 0.39998643951975105, 0.2...              2\n",
      "160  [[-0.9162021541613891, 0.20073783465193695, 0....              2\n",
      "161  [[-0.671387869150753, 0.39998643951975105, 0.2...              2\n",
      "171  [[-1.8989778378879687, 2.5088177983703224, 0.8...              0\n",
      "172  [[1.4308632317717622, 2.4975254766390544, -0.7...              0\n",
      "182  [[-1.5596796941446023, 0.2536804729353711, 1.3...              0\n",
      "183  [[-0.6887591980019905, 0.6032038668880669, 0.9...              0\n",
      "193  [[-0.46881921391159853, -0.5718406469410432, -...              1\n",
      "194  [[0.0012300998753839707, 0.7945727784215946, -...              1\n",
      "204  [[-0.7915628218100028, -0.7720629101660486, 2....              1\n",
      "205  [[-0.29490221369437813, -1.5473451288260294, -...              1\n",
      "215  [[-0.5197215910154113, 0.2912065723187065, 0.0...              2\n",
      "216  [[-0.27755483375382767, 0.0736153840003123, 0....              2\n",
      "226  [[-0.5197215910154113, 0.2912065723187065, 0.0...              2\n",
      "227  [[-0.27755483375382767, 0.0736153840003123, 0....              2\n",
      "238  [[1.6144672906518485, 2.007226825405899, -0.94...              0\n",
      "239  [[-3.6340911216450222, 2.4191754325979944, 1.8...              0\n",
      "250  [[1.8606490206201582, 2.4224902877437944, 0.23...              0\n",
      "251  [[-2.550255575924285, 0.9280119128363008, 1.01...              0\n",
      "262  [[-0.11144890861189963, -1.3424991905047203, -...              1\n",
      "263  [[-0.7534415188232086, -0.5805150924558018, 0....              1\n",
      "274  [[0.8379616609173215, 1.6501747569399747, -1.8...              1\n",
      "275  [[-0.3822573845302161, -0.7901941573091099, -2...              1\n",
      "286  [[-0.01672499854054074, -0.5163033866401274, 0...              2\n",
      "287  [[-0.7151808472320134, 0.24898119060516596, 0....              2\n",
      "298  [[-0.01672499854054074, -0.5163033866401274, 0...              2\n",
      "299  [[-0.7151808472320134, 0.24898119060516596, 0....              2\n",
      "train_X:\n",
      "[[[-1.5695887e+00  2.1484096e-01  3.3761808e-01 ... -4.4781275e-02\n",
      "   -1.0880531e-01  1.2134084e+00]\n",
      "  [-1.7620863e+00  2.5406614e-01  2.8659788e-01 ... -4.4160429e-02\n",
      "   -1.0007611e-01  1.2204227e+00]\n",
      "  [-1.9278647e+00  3.4490213e-01  2.9215914e-01 ... -4.2506948e-02\n",
      "   -9.0269350e-02  1.2279077e+00]\n",
      "  ...\n",
      "  [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ...  0.0000000e+00\n",
      "    0.0000000e+00  0.0000000e+00]\n",
      "  [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ...  0.0000000e+00\n",
      "    0.0000000e+00  0.0000000e+00]\n",
      "  [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ...  0.0000000e+00\n",
      "    0.0000000e+00  0.0000000e+00]]\n",
      "\n",
      " [[ 8.5073423e-01  9.5665294e-01 -1.1791586e+00 ...  1.6041377e-01\n",
      "   -8.5695781e-02  1.5699049e+00]\n",
      "  [ 1.2131977e+00  9.8995286e-01 -1.1371959e+00 ...  1.6255398e-01\n",
      "   -8.9243010e-02  1.5604852e+00]\n",
      "  [ 1.6911819e+00  9.3972135e-01 -3.3754858e-01 ...  1.6505527e-01\n",
      "   -9.2739791e-02  1.5514193e+00]\n",
      "  ...\n",
      "  [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ...  0.0000000e+00\n",
      "    0.0000000e+00  0.0000000e+00]\n",
      "  [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ...  0.0000000e+00\n",
      "    0.0000000e+00  0.0000000e+00]\n",
      "  [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ...  0.0000000e+00\n",
      "    0.0000000e+00  0.0000000e+00]]\n",
      "\n",
      " [[-2.5598216e+00  1.7755334e+00  1.3196601e+00 ...  1.9680673e-01\n",
      "   -6.1708201e-02  1.2734401e+00]\n",
      "  [-2.9904976e+00  1.7787939e+00  1.3526976e+00 ...  1.9801138e-01\n",
      "   -6.2836766e-02  1.2907910e+00]\n",
      "  [-3.3470299e+00  1.7071015e+00  1.4555908e+00 ...  2.0303982e-01\n",
      "   -6.6486850e-02  1.3100580e+00]\n",
      "  ...\n",
      "  [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ...  0.0000000e+00\n",
      "    0.0000000e+00  0.0000000e+00]\n",
      "  [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ...  0.0000000e+00\n",
      "    0.0000000e+00  0.0000000e+00]\n",
      "  [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ...  0.0000000e+00\n",
      "    0.0000000e+00  0.0000000e+00]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[-8.7494707e-01 -3.3148220e-01  1.7146960e-01 ... -1.2881995e+00\n",
      "    1.1000171e+00 -1.5433064e-01]\n",
      "  [-8.7732357e-01 -2.9177970e-01  1.9663250e-01 ... -1.2311655e+00\n",
      "    1.0842353e+00 -1.7376560e-01]\n",
      "  [-8.8482791e-01 -2.4835697e-01  2.4845941e-01 ... -1.1727026e+00\n",
      "    1.0678520e+00 -1.9314906e-01]\n",
      "  ...\n",
      "  [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ...  0.0000000e+00\n",
      "    0.0000000e+00  0.0000000e+00]\n",
      "  [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ...  0.0000000e+00\n",
      "    0.0000000e+00  0.0000000e+00]\n",
      "  [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ...  0.0000000e+00\n",
      "    0.0000000e+00  0.0000000e+00]]\n",
      "\n",
      " [[ 3.2676458e-01 -2.1426880e-01  6.0787819e-02 ... -2.7251432e+00\n",
      "    1.4799240e+00  9.9125123e-01]\n",
      "  [ 3.0811971e-01 -2.3776521e-01  1.5556772e-02 ... -2.7157083e+00\n",
      "    1.4780608e+00  9.6790200e-01]\n",
      "  [ 2.8674498e-01 -2.5764692e-01 -2.7957435e-03 ... -2.7044842e+00\n",
      "    1.4757936e+00  9.4145429e-01]\n",
      "  ...\n",
      "  [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ...  0.0000000e+00\n",
      "    0.0000000e+00  0.0000000e+00]\n",
      "  [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ...  0.0000000e+00\n",
      "    0.0000000e+00  0.0000000e+00]\n",
      "  [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ...  0.0000000e+00\n",
      "    0.0000000e+00  0.0000000e+00]]\n",
      "\n",
      " [[ 1.1940233e+00  4.1113275e-01  2.2743616e-01 ... -2.0259380e+00\n",
      "    1.3749585e+00  1.7388685e-01]\n",
      "  [ 1.1575133e+00  4.4216216e-01  2.3414728e-01 ... -2.0663702e+00\n",
      "    1.3834435e+00  2.2352463e-01]\n",
      "  [ 1.1276224e+00  4.7887602e-01  2.5415665e-01 ... -2.1067472e+00\n",
      "    1.3918949e+00  2.7322376e-01]\n",
      "  ...\n",
      "  [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ...  0.0000000e+00\n",
      "    0.0000000e+00  0.0000000e+00]\n",
      "  [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ...  0.0000000e+00\n",
      "    0.0000000e+00  0.0000000e+00]\n",
      "  [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ...  0.0000000e+00\n",
      "    0.0000000e+00  0.0000000e+00]]]\n",
      "train_y:\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]\n",
      "test_X:\n",
      "[[[ 7.8941047e-01 -7.2666717e-01 -2.8022975e-01 ...  5.3228694e-01\n",
      "   -2.4156745e-01  8.0575866e-01]\n",
      "  [ 1.0764475e+00 -6.1405611e-01 -3.0754665e-01 ...  5.2643400e-01\n",
      "   -2.4105582e-01  7.9804486e-01]\n",
      "  [ 1.2319533e+00 -5.5900997e-01 -3.4141266e-01 ...  5.2045763e-01\n",
      "   -2.4052776e-01  7.9047883e-01]\n",
      "  ...\n",
      "  [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ...  0.0000000e+00\n",
      "    0.0000000e+00  0.0000000e+00]\n",
      "  [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ...  0.0000000e+00\n",
      "    0.0000000e+00  0.0000000e+00]\n",
      "  [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ...  0.0000000e+00\n",
      "    0.0000000e+00  0.0000000e+00]]\n",
      "\n",
      " [[ 1.1008683e+00 -8.9868337e-02  6.9236301e-02 ...  3.8674268e-01\n",
      "   -2.0184147e+00  8.8270611e-01]\n",
      "  [ 1.2481254e+00 -3.2374211e-02 -1.3838467e-02 ...  3.8404188e-01\n",
      "   -2.0208733e+00  8.7315589e-01]\n",
      "  [ 1.3539121e+00  6.2687173e-02 -3.1640194e-02 ...  3.7461644e-01\n",
      "   -2.0243320e+00  8.6550242e-01]\n",
      "  ...\n",
      "  [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ...  0.0000000e+00\n",
      "    0.0000000e+00  0.0000000e+00]\n",
      "  [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ...  0.0000000e+00\n",
      "    0.0000000e+00  0.0000000e+00]\n",
      "  [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ...  0.0000000e+00\n",
      "    0.0000000e+00  0.0000000e+00]]\n",
      "\n",
      " [[-4.4868672e-01 -4.8498407e-01  9.4984037e-01 ...  3.0495477e-01\n",
      "    2.2767483e-01 -4.7013655e-02]\n",
      "  [-4.0379703e-01 -3.9822268e-01  9.2266411e-01 ...  3.0483663e-01\n",
      "    2.3655418e-01 -5.6272626e-02]\n",
      "  [-3.5203677e-01 -3.1809029e-01  8.8517946e-01 ...  3.0438909e-01\n",
      "    2.4611869e-01 -7.0174493e-02]\n",
      "  ...\n",
      "  [-5.1119888e-01 -1.2328706e+00 -1.3914036e+00 ...  1.4730607e-01\n",
      "    3.3353749e-01  1.5826705e-01]\n",
      "  [-5.1472670e-01 -1.2057370e+00 -1.2990870e+00 ...  1.4659430e-01\n",
      "    3.3794749e-01  1.2882185e-01]\n",
      "  [-5.1053929e-01 -1.1785502e+00 -1.3213745e+00 ...  1.4588253e-01\n",
      "    3.4235749e-01  9.9376649e-02]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[-7.1518087e-01  2.4898119e-01  1.3252854e-01 ...  5.3919685e-01\n",
      "    5.8338654e-01 -3.3610716e-01]\n",
      "  [-7.4579370e-01  2.4577844e-01  1.5475872e-01 ...  5.3511822e-01\n",
      "    5.8630228e-01 -3.4661365e-01]\n",
      "  [-7.6971072e-01  2.2805920e-01  1.7239633e-01 ...  5.3100348e-01\n",
      "    5.8923668e-01 -3.5719854e-01]\n",
      "  ...\n",
      "  [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ...  0.0000000e+00\n",
      "    0.0000000e+00  0.0000000e+00]\n",
      "  [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ...  0.0000000e+00\n",
      "    0.0000000e+00  0.0000000e+00]\n",
      "  [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ...  0.0000000e+00\n",
      "    0.0000000e+00  0.0000000e+00]]\n",
      "\n",
      " [[-1.6724998e-02 -5.1630336e-01  5.7357080e-02 ... -1.7247668e-01\n",
      "    8.4306633e-01 -6.6994059e-01]\n",
      "  [ 1.7521901e-02 -4.9749428e-01  4.2900208e-02 ... -2.1636397e-01\n",
      "    8.5804713e-01 -6.6483498e-01]\n",
      "  [ 4.3540452e-02 -4.8283729e-01 -1.1842641e-03 ... -2.6195431e-01\n",
      "    8.7373835e-01 -6.6107666e-01]\n",
      "  ...\n",
      "  [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ...  0.0000000e+00\n",
      "    0.0000000e+00  0.0000000e+00]\n",
      "  [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ...  0.0000000e+00\n",
      "    0.0000000e+00  0.0000000e+00]\n",
      "  [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ...  0.0000000e+00\n",
      "    0.0000000e+00  0.0000000e+00]]\n",
      "\n",
      " [[-7.1518087e-01  2.4898119e-01  1.3252854e-01 ...  5.3919685e-01\n",
      "    5.8338654e-01 -3.3610716e-01]\n",
      "  [-7.4579370e-01  2.4577844e-01  1.5475872e-01 ...  5.3511822e-01\n",
      "    5.8630228e-01 -3.4661365e-01]\n",
      "  [-7.6971072e-01  2.2805920e-01  1.7239633e-01 ...  5.3100348e-01\n",
      "    5.8923668e-01 -3.5719854e-01]\n",
      "  ...\n",
      "  [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ...  0.0000000e+00\n",
      "    0.0000000e+00  0.0000000e+00]\n",
      "  [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ...  0.0000000e+00\n",
      "    0.0000000e+00  0.0000000e+00]\n",
      "  [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ...  0.0000000e+00\n",
      "    0.0000000e+00  0.0000000e+00]]]\n",
      "test_y:\n",
      "[0 0 1 1 2 2 0 0 1 1 2 2 0 0 0 0 1 1 1 1 2 2 2 2 0 0 0 0 1 1 1 1 2 2 2 2 0\n",
      " 0 0 0 1 1 1 1 2 2 2 2]\n"
     ]
    }
   ],
   "source": [
    "# Split the data into train data and test data, to avoid train data mixed into test data spilit it by time series\n",
    "train_df = df_concat[(df_concat[\"window\"] + 1) / df_concat[\"windows_count\"] < 0.8].copy()\n",
    "train_df.drop([\"window\", \"windows_count\"], axis=1, inplace=True)\n",
    "test_df = df_concat[df_concat[\"window\"] / df_concat[\"windows_count\"] >= 0.8].copy()\n",
    "test_df.drop([\"window\", \"windows_count\"], axis=1, inplace=True)\n",
    "print(train_df)\n",
    "print(test_df)\n",
    "\n",
    "# Define a function to divide the data into X and y\n",
    "def divide_Xy(df, oversample=True):\n",
    "    data = df.to_numpy()\n",
    "    X = data[:, :-1]\n",
    "    y = data[:, 1].astype(np.int32)\n",
    "    if oversample:\n",
    "        ros = RandomOverSampler(random_state=0)\n",
    "        X, y = ros.fit_resample(X, y)\n",
    "    X = np.stack([item[0] for item in X])\n",
    "    X = X.astype(np.float32)\n",
    "    return X, y\n",
    "\n",
    "# Divide the train data and test data into X and y\n",
    "train_X, train_y = divide_Xy(train_df)\n",
    "test_X, test_y = divide_Xy(test_df, oversample=False)\n",
    "\n",
    "# Print the data\n",
    "print(\"train_X:\")\n",
    "print(train_X)\n",
    "print(\"train_y:\")\n",
    "print(train_y)\n",
    "print(\"test_X:\")\n",
    "print(test_X)\n",
    "print(\"test_y:\")\n",
    "print(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2944079",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:cpu\n",
      "Epoch 1 loss: 0.03257086282377845\n",
      "Epoch 2 loss: 0.024134690160149925\n",
      "Epoch 3 loss: 0.018302378890750644\n",
      "Epoch 4 loss: 0.017467809421522124\n",
      "Epoch 5 loss: 0.017710431739016697\n",
      "Epoch 6 loss: 0.017736034350352245\n",
      "Epoch 7 loss: 0.017415566218865884\n",
      "Epoch 8 loss: 0.017388463288814097\n",
      "Epoch 9 loss: 0.017391754700256896\n",
      "Epoch 10 loss: 0.01739498570158675\n",
      "Finished Training\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        16\n",
      "           1       1.00      1.00      1.00        16\n",
      "           2       1.00      1.00      1.00        16\n",
      "\n",
      "    accuracy                           1.00        48\n",
      "   macro avg       1.00      1.00      1.00        48\n",
      "weighted avg       1.00      1.00      1.00        48\n",
      "\n",
      "Confusion matrix: \n",
      "[[16  0  0]\n",
      " [ 0 16  0]\n",
      " [ 0  0 16]]\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device:{device}\")\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(13, 32, 3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(32, 64, 3, padding=1)\n",
    "        self.pool = nn.MaxPool1d(2, 2)\n",
    "        self.fc1 = nn.Linear(64 * 50, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 3)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.relu = nn.ReLU()\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.relu(self.conv1(x)))\n",
    "        x = self.pool(self.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.softmax(self.fc3(x))\n",
    "        return x\n",
    "    def predict(self, X):\n",
    "        X = torch.tensor(X, dtype=torch.float32, device=device)\n",
    "        with torch.no_grad():\n",
    "            outputs = self(X)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "        return predicted.cpu().numpy()\n",
    "\n",
    "# Train the model with train_X and train_y\n",
    "def train_model(model, train_X, train_y, epochs=10, batch_size=32, lr=0.001):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    model.to(device)\n",
    "    train_X = torch.tensor(train_X, dtype=torch.float32, device=device)\n",
    "    train_y = torch.tensor(train_y, dtype=torch.long, device=device)\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for i in range(0, len(train_X), batch_size):\n",
    "            inputs = train_X[i:i+batch_size]\n",
    "            labels = train_y[i:i+batch_size]\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        print(f\"Epoch {epoch + 1} loss: {running_loss / len(train_X)}\")\n",
    "    print(\"Finished Training\")\n",
    "    return model\n",
    "\n",
    "# Call train_model to train the model\n",
    "cnn_model = CNN()\n",
    "train_X_CNN = train_X.transpose(0, 2, 1)\n",
    "train_y_CNN = train_y\n",
    "test_X_CNN = test_X.transpose(0, 2, 1)\n",
    "test_y_CNN = test_y\n",
    "cnn_model = train_model(cnn_model, train_X_CNN, train_y_CNN)\n",
    "def evaluate_model(model, X, y):\n",
    "    predicted_y = model.predict(X)\n",
    "    print(classification_report(y, predicted_y))\n",
    "\n",
    "    confusion_mat = confusion_matrix(y, predicted_y)\n",
    "    print(f\"Confusion matrix: \\n{confusion_mat}\")\n",
    "evaluate_model(cnn_model, test_X_CNN, test_y_CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ab04044",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 loss: 0.033358786020193015\n",
      "Epoch 2 loss: 0.028978600695326522\n",
      "Epoch 3 loss: 0.023785125027905713\n",
      "Epoch 4 loss: 0.01963886967650405\n",
      "Epoch 5 loss: 0.01671138251418466\n",
      "Epoch 6 loss: 0.013829115140545476\n",
      "Epoch 7 loss: 0.010991009069724125\n",
      "Epoch 8 loss: 0.008745069607152595\n",
      "Epoch 9 loss: 0.0072075734670097764\n",
      "Epoch 10 loss: 0.005997003953870352\n",
      "Epoch 11 loss: 0.004914892560525521\n",
      "Epoch 12 loss: 0.003965228800200396\n",
      "Epoch 13 loss: 0.0030082213558122383\n",
      "Epoch 14 loss: 0.002010744679635307\n",
      "Epoch 15 loss: 0.0011867572241966184\n",
      "Epoch 16 loss: 0.0006000178851405377\n",
      "Epoch 17 loss: 0.00038395701490158033\n",
      "Epoch 18 loss: 0.00025230157818343187\n",
      "Epoch 19 loss: 0.00015018785845379235\n",
      "Epoch 20 loss: 9.394036870772807e-05\n",
      "Epoch 21 loss: 6.798932998470525e-05\n",
      "Epoch 22 loss: 5.39920198403903e-05\n",
      "Epoch 23 loss: 4.38128820508257e-05\n",
      "Epoch 24 loss: 3.564559203270521e-05\n",
      "Epoch 25 loss: 2.944252107251511e-05\n",
      "Epoch 26 loss: 2.4852096832038517e-05\n",
      "Epoch 27 loss: 2.1426003022006136e-05\n",
      "Epoch 28 loss: 1.8799661431641965e-05\n",
      "Epoch 29 loss: 1.6710539876749772e-05\n",
      "Epoch 30 loss: 1.499294644868765e-05\n",
      "Epoch 31 loss: 1.3550393002030129e-05\n",
      "Epoch 32 loss: 1.2323460281435359e-05\n",
      "Epoch 33 loss: 1.1272237430717837e-05\n",
      "Epoch 34 loss: 1.0364702665446293e-05\n",
      "Epoch 35 loss: 9.575812483645833e-06\n",
      "Finished Training\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      1.00      0.97        16\n",
      "           1       1.00      0.94      0.97        16\n",
      "           2       1.00      1.00      1.00        16\n",
      "\n",
      "    accuracy                           0.98        48\n",
      "   macro avg       0.98      0.98      0.98        48\n",
      "weighted avg       0.98      0.98      0.98        48\n",
      "\n",
      "Confusion matrix: \n",
      "[[16  0  0]\n",
      " [ 1 15  0]\n",
      " [ 0  0 16]]\n"
     ]
    }
   ],
   "source": [
    "class GRU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GRU, self).__init__()\n",
    "        self.gru = nn.GRU(13, 64, 1, batch_first=True)\n",
    "        self.fc1 = nn.Linear(64, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 3)\n",
    "    def forward(self, x):\n",
    "        x, _ = self.gru(x)\n",
    "        x = self.fc1(x[:, -1, :])\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    def predict(self, X):\n",
    "        X = torch.tensor(X, dtype=torch.float32, device=device)\n",
    "        with torch.no_grad():\n",
    "            outputs = self(X)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "        return predicted.cpu().numpy()\n",
    "gru_model = GRU()\n",
    "train_X_GRU = train_X\n",
    "train_y_GRU = train_y\n",
    "test_X_GRU = test_X\n",
    "test_y_GRU = test_y\n",
    "gru_model = train_model(gru_model, train_X_GRU, train_y_GRU, epochs=35)\n",
    "evaluate_model(gru_model, test_X_GRU, test_y_GRU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d58837b",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 loss: 0.03423549516780956\n",
      "Epoch 2 loss: 0.02802096938227748\n",
      "Epoch 3 loss: 0.016534577350358706\n",
      "Epoch 4 loss: 0.014368221096627347\n",
      "Epoch 5 loss: 0.008691579103469849\n",
      "Epoch 6 loss: 0.004215637356177106\n",
      "Epoch 7 loss: 0.002423203491547086\n",
      "Epoch 8 loss: 0.001629942032793822\n",
      "Epoch 9 loss: 0.00015755472830622584\n",
      "Epoch 10 loss: 0.00019409443169983255\n",
      "Epoch 11 loss: 9.942791744047222e-05\n",
      "Epoch 12 loss: 3.792264341190038e-05\n",
      "Epoch 13 loss: 2.0982172163958484e-05\n",
      "Epoch 14 loss: 1.572139333327393e-05\n",
      "Epoch 15 loss: 1.2178239410639287e-05\n",
      "Epoch 16 loss: 9.616338354897232e-06\n",
      "Epoch 17 loss: 7.960937977144453e-06\n",
      "Epoch 18 loss: 6.858939495695931e-06\n",
      "Epoch 19 loss: 6.057832096147493e-06\n",
      "Epoch 20 loss: 5.436365713935916e-06\n",
      "Epoch 21 loss: 4.934489576650473e-06\n",
      "Epoch 22 loss: 4.516468834481202e-06\n",
      "Epoch 23 loss: 4.158917650161416e-06\n",
      "Epoch 24 loss: 3.8470996039429776e-06\n",
      "Epoch 25 loss: 3.572279793999015e-06\n",
      "Epoch 26 loss: 3.3266800659606945e-06\n",
      "Epoch 27 loss: 3.1076150588813944e-06\n",
      "Epoch 28 loss: 2.9108597356081427e-06\n",
      "Epoch 29 loss: 2.7334623187958676e-06\n",
      "Epoch 30 loss: 2.5733150257954484e-06\n",
      "Finished Training\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.94      0.91        16\n",
      "           1       0.93      0.88      0.90        16\n",
      "           2       1.00      1.00      1.00        16\n",
      "\n",
      "    accuracy                           0.94        48\n",
      "   macro avg       0.94      0.94      0.94        48\n",
      "weighted avg       0.94      0.94      0.94        48\n",
      "\n",
      "Confusion matrix: \n",
      "[[15  1  0]\n",
      " [ 2 14  0]\n",
      " [ 0  0 16]]\n"
     ]
    }
   ],
   "source": [
    "class CNN_GRU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN_GRU, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(13, 32, 3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(32, 64, 3, padding=1)\n",
    "        self.pool = nn.MaxPool1d(2, 2)\n",
    "        self.gru = nn.GRU(64, 64, 2, batch_first=True)\n",
    "        self.fc1 = nn.Linear(64, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 3)\n",
    "        self.relu = nn.ReLU()\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.relu(self.conv1(x)))\n",
    "        x = self.pool(self.relu(self.conv2(x)))\n",
    "        x = x.transpose(2, 1)\n",
    "        x, _ = self.gru(x)\n",
    "        x = self.fc1(x[:, -1, :])\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    def predict(self, X):\n",
    "        X = torch.tensor(X, dtype=torch.float32, device=device)\n",
    "        with torch.no_grad():\n",
    "            outputs = self(X)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "        return predicted.cpu().numpy()\n",
    "\n",
    "cnn_gru_model = CNN_GRU()\n",
    "train_X_CNN_GRU = train_X_CNN\n",
    "train_y_CNN_GRU = train_y_CNN\n",
    "test_X_CNN_GRU = test_X_CNN\n",
    "test_y_CNN_GRU = test_y_CNN\n",
    "cnn_gru_model = train_model(cnn_gru_model, train_X_CNN_GRU, train_y_CNN_GRU, epochs=30)\n",
    "evaluate_model(cnn_gru_model, test_X_CNN_GRU, test_y_CNN_GRU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592e5899",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
