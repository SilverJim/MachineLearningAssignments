{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "94c9a1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the required machine learning libraries and models\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score, classification_report, confusion_matrix\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d48d3bbe",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from data path: ./data\n"
     ]
    }
   ],
   "source": [
    "# All the data should put into the data folder\n",
    "data_filename = 'data'\n",
    "\n",
    "if sys.modules.get(\"google.colab\") is None:\n",
    "    data_path_prefix = \".\"\n",
    "else:\n",
    "    from google.colab import drive\n",
    "    drive.mount(\"/content/drive\")\n",
    "    data_path_prefix = \"/content/drive/MyDrive/MachineLearningAssignments/Assignment4\"\n",
    "\n",
    "data_path = f\"{data_path_prefix}/{data_filename}\"\n",
    "\n",
    "print(f\"Loading data from data path: {data_path}\")\n",
    "\n",
    "# Define the exercise types\n",
    "data_sets = [\n",
    "   {\"path\": \"Jumping_Jack_x10\", \"name\": \"Jumping_Jack\"}, \n",
    "   {\"path\": \"Lunges_x10\", \"name\": \"Lunges\"}, \n",
    "   {\"path\": \"Squat_x10\", \"name\": \"Squat\"}, \n",
    "   {\"path\": \"Jumping_Jack_x10_new\", \"name\": \"Jumping_Jack_new\"}, \n",
    "   {\"path\": \"Lunges_x10_new\", \"name\": \"Lunges_new\"}, \n",
    "   {\"path\": \"Squat_x10\", \"name\": \"Squat_new\"}, \n",
    "]\n",
    "\n",
    "# Define the data sets\n",
    "data_set = [(\"Accelerometer.csv\", \"Accelerometer\"), (\"TotalAcceleration.csv\", \"TotalAcceleration\"), (\"Orientation.csv\", \"Orientation\")]\n",
    "\n",
    "df_list = []\n",
    "\n",
    "# In all the data set after \"time\" is removed, only \"seconds_elapsed\" and \"data_set\" are non-value columns\n",
    "non_value_columns = [\"seconds_elapsed\", \"data_set\"]\n",
    "non_value_columns_set = set(non_value_columns)\n",
    "# read every dataset in every exercise type\n",
    "for file_name, data_set_name in data_set:\n",
    "    temp_df_list = []\n",
    "    for data_set in data_sets:\n",
    "        df = pd.read_csv(f\"{data_path}/{data_set['path']}/{file_name}\")\n",
    "        # There is \"seconds_elapsed\" so \"time\" is not necessary\n",
    "        df.drop(\"time\", axis=1, inplace=True)\n",
    "        # add the data_set column according to which exercise the data from\n",
    "        df['data_set'] = data_set['name']\n",
    "        # Rename the columns according to which dataset the data from to avoid same column\n",
    "        for column in df.columns:\n",
    "            if column not in non_value_columns_set:\n",
    "                df.rename(columns={column: f\"{data_set_name}_{column}\"}, inplace=True)\n",
    "        value_columns = list(set(df.columns) - non_value_columns_set)\n",
    "        # Noise Reduction: calculate rolling mean for all the value columns\n",
    "        df[value_columns] = df[value_columns].rolling(50).mean()\n",
    "        # rolling mean create some NaN values, so we should drop them\n",
    "        df.dropna(inplace=True)\n",
    "        # remove the data at beginning and end of each exercise, as the excercise has not started or has ended\n",
    "        df = df[(df[\"seconds_elapsed\"].quantile(0.1) < df[\"seconds_elapsed\"]) & (df['seconds_elapsed'] < df['seconds_elapsed'].quantile(0.9))]\n",
    "        temp_df_list.append(df)\n",
    "    # combine dataframe from all the exercise types together and put it in to the df_list\n",
    "    df_list.append(pd.concat(temp_df_list))\n",
    "    \n",
    "# Normalization\n",
    "for i in range(len(df_list)):\n",
    "    df = df_list[i]\n",
    "    value_columns = list(set(df.columns) - non_value_columns_set)\n",
    "    scaler = ColumnTransformer(\n",
    "        [\n",
    "            ('standard_scaler', StandardScaler(), value_columns),\n",
    "            ('other', 'passthrough', non_value_columns)\n",
    "\n",
    "        ],\n",
    "    )\n",
    "    scaled_data = scaler.fit_transform(df)\n",
    "    df_list[i] = pd.DataFrame(scaled_data, columns=value_columns + non_value_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ebbc6abb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Jumping_Jack': 4.1496669921875, 'Lunges': 5.907152587890625, 'Squat': 5.70484130859375, 'Jumping_Jack_new': 4.027690185546875, 'Lunges_new': 6.39675537109375, 'Squat_new': 5.70484130859375} {'Jumping_Jack': 3.1924683352539063, 'Lunges': 4.997698285205079, 'Squat': 4.79562354765625, 'Jumping_Jack_new': 3.1396086245605472, 'Lunges_new': 5.482244061572265, 'Squat_new': 4.79562354765625}\n",
      "{'Jumping_Jack': 4.1496669921875, 'Lunges': 5.907152587890625, 'Squat': 5.70484130859375, 'Jumping_Jack_new': 4.027690185546875, 'Lunges_new': 6.39675537109375, 'Squat_new': 5.70484130859375} {'Jumping_Jack': 2.8377496313368056, 'Lunges': 4.442398475737848, 'Squat': 4.2627764868055555, 'Jumping_Jack_new': 2.7907632218315976, 'Lunges_new': 4.8731058325086805, 'Squat_new': 4.2627764868055555}\n",
      "{'Jumping_Jack': 4.1496669921875, 'Lunges': 5.907152587890625, 'Squat': 5.70484130859375, 'Jumping_Jack_new': 4.027690185546875, 'Lunges_new': 6.39675537109375, 'Squat_new': 5.70484130859375} {'Jumping_Jack': 2.553974668203125, 'Lunges': 3.998158628164063, 'Squat': 3.8364988381250003, 'Jumping_Jack_new': 2.5116868996484376, 'Lunges_new': 4.3857952492578125, 'Squat_new': 3.8364988381250003}\n",
      "{'Jumping_Jack': 4.1496669921875, 'Lunges': 5.907152587890625, 'Squat': 5.70484130859375, 'Jumping_Jack_new': 4.027690185546875, 'Lunges_new': 6.39675537109375, 'Squat_new': 5.70484130859375} {'Jumping_Jack': 2.321795152911932, 'Lunges': 3.63468966196733, 'Squat': 3.487726216477273, 'Jumping_Jack_new': 2.2833517269531254, 'Lunges_new': 3.987086590234375, 'Squat_new': 3.487726216477273}\n",
      "{'Jumping_Jack': 4.1496669921875, 'Lunges': 5.907152587890625, 'Squat': 5.70484130859375, 'Jumping_Jack_new': 4.027690185546875, 'Lunges_new': 6.39675537109375, 'Squat_new': 5.70484130859375} {'Jumping_Jack': 2.128312223502604, 'Lunges': 3.331798856803386, 'Squat': 3.197082365104167, 'Jumping_Jack_new': 2.093072416373698, 'Lunges_new': 3.65482937438151, 'Squat_new': 3.197082365104167}\n"
     ]
    }
   ],
   "source": [
    "# Window by dividing the data of each exercise into eaqual parts to apply data augmentation we use several different window sizes\n",
    "\n",
    "# If the length of the array is less than the target length, pad it with zeros and make the original array at the beginning\n",
    "def pad(original_series, target_length):\n",
    "    new_series = original_series.copy()\n",
    "    for i in range(original_series.size):\n",
    "        line = original_series[i]\n",
    "        original_length = len(line)\n",
    "        if original_length > target_length:\n",
    "            new_line = line[:target_length]\n",
    "        else:\n",
    "            new_line = np.pad(line, ((0, target_length - original_length), (0, 0)), mode=\"constant\")\n",
    "        new_series[i] = new_line\n",
    "    return new_series\n",
    "\n",
    "windows_count_list = [8, 9, 10, 11, 12]\n",
    "df_concat_list = []\n",
    "for windows_count in windows_count_list:\n",
    "    seconds_elapsed_min = {}\n",
    "    windows_size = {}\n",
    "    for item in data_sets:\n",
    "        data_set = item[\"name\"]\n",
    "        seconds_elapsed_min[data_set] = min((df[df[\"data_set\"] == data_set][\"seconds_elapsed\"].min() for df in df_list))\n",
    "        seconds_elapsed_max = max((df[df[\"data_set\"] == data_set][\"seconds_elapsed\"].max() for df in df_list)) + 0.0000001\n",
    "        windows_size[data_set] = (seconds_elapsed_max - seconds_elapsed_min[data_set]) / windows_count\n",
    "    print(seconds_elapsed_min, windows_size)\n",
    "\n",
    "    # process window and calculate min, max, mean, std value of each window\n",
    "    count_after_padding = 200\n",
    "    df_list_temp = []\n",
    "    for i, df in enumerate(df_list):\n",
    "        df = df.copy()\n",
    "        df[\"window\"] = df[[\"seconds_elapsed\", \"data_set\"]].apply(lambda row: math.floor((row[\"seconds_elapsed\"] - seconds_elapsed_min[row[\"data_set\"]]) / windows_size[row[\"data_set\"]]), axis=1)\n",
    "        df = df.sort_values([\"data_set\", \"window\", \"seconds_elapsed\"])\n",
    "        df.drop(\"seconds_elapsed\", axis=1, inplace=True)\n",
    "        df = (\n",
    "            df\n",
    "            .groupby([\"data_set\", \"window\"], group_keys=True)\n",
    "            .apply(lambda x: np.array(x[[column for column in df.columns if column not in {\"data_set\", \"window\"}]]))\n",
    "        ).to_frame()\n",
    "        df[0] = df.apply(lambda x: pad(x, count_after_padding))\n",
    "        df.rename(columns={0: i}, inplace=True)\n",
    "        df_list_temp.append(df)\n",
    "    # join the dataframe from all the dataset together according to index \"windows\" and \"data_set\"\n",
    "    df_concat_temp = pd.concat(df_list_temp, axis=1)\n",
    "    df_concat_temp[\"windows_count\"] = windows_count\n",
    "    df_concat_list.append(df_concat_temp)\n",
    "df_concat = pd.concat(df_concat_list)\n",
    "df_concat.reset_index(inplace=True)\n",
    "# Make the data_set the last column\n",
    "data_set_mappping = {\n",
    "    \"Jumping_Jack\": 0,\n",
    "    \"Lunges\": 1,\n",
    "    \"Squat\": 2,\n",
    "    \"Jumping_Jack_new\": 0,\n",
    "    \"Lunges_new\": 1,\n",
    "    \"Squat_new\": 2,\n",
    "}\n",
    "df_concat[\"exercise_type\"] = df_concat[\"data_set\"].map(data_set_mappping)\n",
    "df_concat.drop([\"data_set\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "19e71e19",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     window  windows_count                                           features  \\\n",
      "0         0              8  [[0.2148409640207338, -1.5695887046081776, 0.3...   \n",
      "1         1              8  [[0.9566529490333389, 0.8507342366849948, -1.1...   \n",
      "2         2              8  [[1.7755334870284505, -2.5598215168907585, 1.3...   \n",
      "3         3              8  [[1.309237396359241, 2.930671137397125, -1.125...   \n",
      "4         4              8  [[0.12381020968409308, -1.2542389201316566, 0....   \n",
      "..      ...            ...                                                ...   \n",
      "295       7             12  [[-0.21426879701074242, 0.3267645829981819, 0....   \n",
      "296       8             12  [[0.4111327570286471, 1.1940232312300132, 0.22...   \n",
      "297       9             12  [[-0.6273576309412399, 0.7831527569642023, 0.1...   \n",
      "298      10             12  [[-0.5163033866401274, -0.01672499854054074, 0...   \n",
      "299      11             12  [[0.24898119060516596, -0.7151808472320134, 0....   \n",
      "\n",
      "     exercise_type  \n",
      "0                0  \n",
      "1                0  \n",
      "2                0  \n",
      "3                0  \n",
      "4                0  \n",
      "..             ...  \n",
      "295              2  \n",
      "296              2  \n",
      "297              2  \n",
      "298              2  \n",
      "299              2  \n",
      "\n",
      "[300 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "df_concat[\"features\"] = df_concat.apply(lambda x:np.concatenate([x[0], x[1], x[2]], axis=1), axis=1)\n",
    "df_concat.drop([0, 1, 2], axis=1, inplace=True)\n",
    "temp = df_concat.pop(\"exercise_type\")\n",
    "df_concat[\"exercise_type\"] = temp\n",
    "print(df_concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "39ad668d",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              features  exercise_type\n",
      "0    [[0.2148409640207338, -1.5695887046081776, 0.3...              0\n",
      "1    [[0.9566529490333389, 0.8507342366849948, -1.1...              0\n",
      "2    [[1.7755334870284505, -2.5598215168907585, 1.3...              0\n",
      "3    [[1.309237396359241, 2.930671137397125, -1.125...              0\n",
      "4    [[0.12381020968409308, -1.2542389201316566, 0....              0\n",
      "..                                                 ...            ...\n",
      "292  [[0.30483245660976754, -0.48708095569365356, 0...              2\n",
      "293  [[0.35358906022672437, -0.5416458186250555, 0....              2\n",
      "294  [[-0.3314822076043692, -0.8749470578231893, 0....              2\n",
      "295  [[-0.21426879701074242, 0.3267645829981819, 0....              2\n",
      "296  [[0.4111327570286471, 1.1940232312300132, 0.22...              2\n",
      "\n",
      "[222 rows x 2 columns]\n",
      "                                              features  exercise_type\n",
      "7    [[-0.7266671600498076, 0.7894104613625587, -0....              0\n",
      "15   [[-0.08986833758746772, 1.1008682937670211, 0....              0\n",
      "23   [[-0.4849840587419749, -0.44868672162222817, 0...              1\n",
      "31   [[-1.489188127084881, -0.5414740784456678, 0.6...              1\n",
      "39   [[-0.6116867500803779, -0.1977959222111066, -0...              2\n",
      "47   [[-0.6116867500803779, -0.1977959222111066, -0...              2\n",
      "56   [[0.17418894750361627, -1.0157741562911824, 0....              0\n",
      "65   [[-0.18874751732615794, -1.0009313999224645, 1...              0\n",
      "74   [[1.5196966449998783, 0.7160646248035505, -1.7...              1\n",
      "83   [[-0.594052200441339, -0.07851421526578999, 0....              1\n",
      "92   [[-0.0389568100502965, -1.0543844711662511, 0....              2\n",
      "101  [[-0.0389568100502965, -1.0543844711662511, 0....              2\n",
      "110  [[1.154669783839552, -2.5527870460628135, 0.70...              0\n",
      "111  [[0.8376978705808282, 1.579484884350927, -1.23...              0\n",
      "120  [[-0.1502559552656285, 0.8649881958396037, 0.2...              0\n",
      "121  [[0.41793621896601024, 2.344917608186814, -0.6...              0\n",
      "130  [[2.1551554939690996, 1.0453044747380378, 1.51...              1\n",
      "131  [[2.2540566214427553, 1.0452960845101857, -2.1...              1\n",
      "140  [[-0.5705216089020205, -0.19113669783656398, -...              1\n",
      "141  [[-0.4741517824648357, -0.043869180459292535, ...              1\n",
      "150  [[0.20073783465193695, -0.9162021541613891, 0....              2\n",
      "151  [[0.39998643951975105, -0.671387869150753, 0.2...              2\n",
      "160  [[0.20073783465193695, -0.9162021541613891, 0....              2\n",
      "161  [[0.39998643951975105, -0.671387869150753, 0.2...              2\n",
      "171  [[2.5088177983703224, -1.8989778378879687, 0.8...              0\n",
      "172  [[2.4975254766390544, 1.4308632317717622, -0.7...              0\n",
      "182  [[0.2536804729353711, -1.5596796941446023, 1.3...              0\n",
      "183  [[0.6032038668880669, -0.6887591980019905, 0.9...              0\n",
      "193  [[-0.5718406469410432, -0.46881921391159853, -...              1\n",
      "194  [[0.7945727784215946, 0.0012300998753839707, -...              1\n",
      "204  [[-0.7720629101660486, -0.7915628218100028, 2....              1\n",
      "205  [[-1.5473451288260294, -0.29490221369437813, -...              1\n",
      "215  [[0.2912065723187065, -0.5197215910154113, 0.0...              2\n",
      "216  [[0.0736153840003123, -0.27755483375382767, 0....              2\n",
      "226  [[0.2912065723187065, -0.5197215910154113, 0.0...              2\n",
      "227  [[0.0736153840003123, -0.27755483375382767, 0....              2\n",
      "238  [[2.007226825405899, 1.6144672906518485, -0.94...              0\n",
      "239  [[2.4191754325979944, -3.6340911216450222, 1.8...              0\n",
      "250  [[2.4224902877437944, 1.8606490206201582, 0.23...              0\n",
      "251  [[0.9280119128363008, -2.550255575924285, 1.01...              0\n",
      "262  [[-1.3424991905047203, -0.11144890861189963, -...              1\n",
      "263  [[-0.5805150924558018, -0.7534415188232086, 0....              1\n",
      "274  [[1.6501747569399747, 0.8379616609173215, -1.8...              1\n",
      "275  [[-0.7901941573091099, -0.3822573845302161, -2...              1\n",
      "286  [[-0.5163033866401274, -0.01672499854054074, 0...              2\n",
      "287  [[0.24898119060516596, -0.7151808472320134, 0....              2\n",
      "298  [[-0.5163033866401274, -0.01672499854054074, 0...              2\n",
      "299  [[0.24898119060516596, -0.7151808472320134, 0....              2\n",
      "train_X:\n",
      "[[[ 2.1484096e-01 -1.5695887e+00  3.3761808e-01 ...  1.2134084e+00\n",
      "    3.6251453e-01  7.4263863e-02]\n",
      "  [ 2.5406614e-01 -1.7620863e+00  2.8659788e-01 ...  1.2204227e+00\n",
      "    3.5053164e-01  6.9193996e-02]\n",
      "  [ 3.4490213e-01 -1.9278647e+00  2.9215914e-01 ...  1.2279077e+00\n",
      "    3.3548000e-01  6.2356185e-02]\n",
      "  ...\n",
      "  [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ...  0.0000000e+00\n",
      "    0.0000000e+00  0.0000000e+00]\n",
      "  [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ...  0.0000000e+00\n",
      "    0.0000000e+00  0.0000000e+00]\n",
      "  [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ...  0.0000000e+00\n",
      "    0.0000000e+00  0.0000000e+00]]\n",
      "\n",
      " [[ 9.5665294e-01  8.5073423e-01 -1.1791586e+00 ...  1.5699049e+00\n",
      "   -2.2509186e-02 -2.0827945e-01]\n",
      "  [ 9.8995286e-01  1.2131977e+00 -1.1371959e+00 ...  1.5604852e+00\n",
      "   -2.0633314e-02 -2.0843072e-01]\n",
      "  [ 9.3972135e-01  1.6911819e+00 -3.3754858e-01 ...  1.5514193e+00\n",
      "   -1.9695623e-02 -2.0918080e-01]\n",
      "  ...\n",
      "  [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ...  0.0000000e+00\n",
      "    0.0000000e+00  0.0000000e+00]\n",
      "  [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ...  0.0000000e+00\n",
      "    0.0000000e+00  0.0000000e+00]\n",
      "  [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ...  0.0000000e+00\n",
      "    0.0000000e+00  0.0000000e+00]]\n",
      "\n",
      " [[ 1.7755334e+00 -2.5598216e+00  1.3196601e+00 ...  1.2734401e+00\n",
      "   -7.4905433e-02 -2.3396128e-01]\n",
      "  [ 1.7787939e+00 -2.9904976e+00  1.3526976e+00 ...  1.2907910e+00\n",
      "   -7.7834897e-02 -2.3655437e-01]\n",
      "  [ 1.7071015e+00 -3.3470299e+00  1.4555908e+00 ...  1.3100580e+00\n",
      "   -8.4741756e-02 -2.4312691e-01]\n",
      "  ...\n",
      "  [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ...  0.0000000e+00\n",
      "    0.0000000e+00  0.0000000e+00]\n",
      "  [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ...  0.0000000e+00\n",
      "    0.0000000e+00  0.0000000e+00]\n",
      "  [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ...  0.0000000e+00\n",
      "    0.0000000e+00  0.0000000e+00]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[-3.3148220e-01 -8.7494707e-01  1.7146960e-01 ... -1.5433064e-01\n",
      "    9.4655621e-01  1.2486560e+00]\n",
      "  [-2.9177970e-01 -8.7732357e-01  1.9663250e-01 ... -1.7376560e-01\n",
      "    8.8930726e-01  1.1885308e+00]\n",
      "  [-2.4835697e-01 -8.8482791e-01  2.4845941e-01 ... -1.9314906e-01\n",
      "    8.2994831e-01  1.1266656e+00]\n",
      "  ...\n",
      "  [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ...  0.0000000e+00\n",
      "    0.0000000e+00  0.0000000e+00]\n",
      "  [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ...  0.0000000e+00\n",
      "    0.0000000e+00  0.0000000e+00]\n",
      "  [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ...  0.0000000e+00\n",
      "    0.0000000e+00  0.0000000e+00]]\n",
      "\n",
      " [[-2.1426880e-01  3.2676458e-01  6.0787819e-02 ...  9.9125123e-01\n",
      "    2.1456964e+00  2.6565492e+00]\n",
      "  [-2.3776521e-01  3.0811971e-01  1.5556772e-02 ...  9.6790200e-01\n",
      "    2.1407132e+00  2.6489344e+00]\n",
      "  [-2.5764692e-01  2.8674498e-01 -2.7957435e-03 ...  9.4145429e-01\n",
      "    2.1346419e+00  2.6397643e+00]\n",
      "  ...\n",
      "  [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ...  0.0000000e+00\n",
      "    0.0000000e+00  0.0000000e+00]\n",
      "  [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ...  0.0000000e+00\n",
      "    0.0000000e+00  0.0000000e+00]\n",
      "  [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ...  0.0000000e+00\n",
      "    0.0000000e+00  0.0000000e+00]]\n",
      "\n",
      " [[ 4.1113275e-01  1.1940233e+00  2.2743616e-01 ...  1.7388685e-01\n",
      "    1.5676309e+00  1.9895476e+00]\n",
      "  [ 4.4216216e-01  1.1575133e+00  2.3414728e-01 ...  2.2352463e-01\n",
      "    1.6036810e+00  2.0286751e+00]\n",
      "  [ 4.7887602e-01  1.1276224e+00  2.5415665e-01 ...  2.7322376e-01\n",
      "    1.6396880e+00  2.0677416e+00]\n",
      "  ...\n",
      "  [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ...  0.0000000e+00\n",
      "    0.0000000e+00  0.0000000e+00]\n",
      "  [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ...  0.0000000e+00\n",
      "    0.0000000e+00  0.0000000e+00]\n",
      "  [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ...  0.0000000e+00\n",
      "    0.0000000e+00  0.0000000e+00]]]\n",
      "train_y:\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]\n",
      "test_X:\n",
      "[[[-7.2666717e-01  7.8941047e-01 -2.8022975e-01 ...  8.0575866e-01\n",
      "   -4.1589221e-01 -5.5057967e-01]\n",
      "  [-6.1405611e-01  1.0764475e+00 -3.0754665e-01 ...  7.9804486e-01\n",
      "   -4.0363172e-01 -5.4208153e-01]\n",
      "  [-5.5900997e-01  1.2319533e+00 -3.4141266e-01 ...  7.9047883e-01\n",
      "   -3.9117759e-01 -5.3344196e-01]\n",
      "  ...\n",
      "  [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ...  0.0000000e+00\n",
      "    0.0000000e+00  0.0000000e+00]\n",
      "  [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ...  0.0000000e+00\n",
      "    0.0000000e+00  0.0000000e+00]\n",
      "  [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ...  0.0000000e+00\n",
      "    0.0000000e+00  0.0000000e+00]]\n",
      "\n",
      " [[-8.9868337e-02  1.1008683e+00  6.9236301e-02 ...  8.8270611e-01\n",
      "    8.8408619e-01 -1.9825505e-02]\n",
      "  [-3.2374211e-02  1.2481254e+00 -1.3838467e-02 ...  8.7315589e-01\n",
      "    8.8863415e-01 -1.6525099e-02]\n",
      "  [ 6.2687173e-02  1.3539121e+00 -3.1640194e-02 ...  8.6550242e-01\n",
      "    9.0321809e-01 -5.7206433e-03]\n",
      "  ...\n",
      "  [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ...  0.0000000e+00\n",
      "    0.0000000e+00  0.0000000e+00]\n",
      "  [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ...  0.0000000e+00\n",
      "    0.0000000e+00  0.0000000e+00]\n",
      "  [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ...  0.0000000e+00\n",
      "    0.0000000e+00  0.0000000e+00]]\n",
      "\n",
      " [[-4.8498407e-01 -4.4868672e-01  9.4984037e-01 ... -4.7013655e-02\n",
      "   -4.5204401e-01 -3.7254941e-01]\n",
      "  [-3.9822268e-01 -4.0379703e-01  9.2266411e-01 ... -5.6272626e-02\n",
      "   -4.6294737e-01 -3.7571251e-01]\n",
      "  [-3.1809029e-01 -3.5203677e-01  8.8517946e-01 ... -7.0174493e-02\n",
      "   -4.7348428e-01 -3.7831372e-01]\n",
      "  ...\n",
      "  [-1.2328706e+00 -5.1119888e-01 -1.3914036e+00 ...  1.5826705e-01\n",
      "   -3.1506774e-01 -2.2866675e-01]\n",
      "  [-1.2057370e+00 -5.1472670e-01 -1.2990870e+00 ...  1.2882185e-01\n",
      "   -3.1544498e-01 -2.2675763e-01]\n",
      "  [-1.1785502e+00 -5.1053929e-01 -1.3213745e+00 ...  9.9376649e-02\n",
      "   -3.1582221e-01 -2.2484851e-01]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 2.4898119e-01 -7.1518087e-01  1.3252854e-01 ... -3.3610716e-01\n",
      "   -1.5267680e+00 -8.9903635e-01]\n",
      "  [ 2.4577844e-01 -7.4579370e-01  1.5475872e-01 ... -3.4661365e-01\n",
      "   -1.5196625e+00 -8.9301419e-01]\n",
      "  [ 2.2805920e-01 -7.6971072e-01  1.7239633e-01 ... -3.5719854e-01\n",
      "   -1.5124875e+00 -8.8693750e-01]\n",
      "  ...\n",
      "  [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ...  0.0000000e+00\n",
      "    0.0000000e+00  0.0000000e+00]\n",
      "  [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ...  0.0000000e+00\n",
      "    0.0000000e+00  0.0000000e+00]\n",
      "  [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ...  0.0000000e+00\n",
      "    0.0000000e+00  0.0000000e+00]]\n",
      "\n",
      " [[-5.1630336e-01 -1.6724998e-02  5.7357080e-02 ... -6.6994059e-01\n",
      "   -4.4591740e-01  3.8204822e-03]\n",
      "  [-4.9749428e-01  1.7521901e-02  4.2900208e-02 ... -6.6483498e-01\n",
      "   -3.9089260e-01  5.3808849e-02]\n",
      "  [-4.8283729e-01  4.3540452e-02 -1.1842641e-03 ... -6.6107666e-01\n",
      "   -3.3450714e-01  1.0561505e-01]\n",
      "  ...\n",
      "  [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ...  0.0000000e+00\n",
      "    0.0000000e+00  0.0000000e+00]\n",
      "  [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ...  0.0000000e+00\n",
      "    0.0000000e+00  0.0000000e+00]\n",
      "  [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ...  0.0000000e+00\n",
      "    0.0000000e+00  0.0000000e+00]]\n",
      "\n",
      " [[ 2.4898119e-01 -7.1518087e-01  1.3252854e-01 ... -3.3610716e-01\n",
      "   -1.5267680e+00 -8.9903635e-01]\n",
      "  [ 2.4577844e-01 -7.4579370e-01  1.5475872e-01 ... -3.4661365e-01\n",
      "   -1.5196625e+00 -8.9301419e-01]\n",
      "  [ 2.2805920e-01 -7.6971072e-01  1.7239633e-01 ... -3.5719854e-01\n",
      "   -1.5124875e+00 -8.8693750e-01]\n",
      "  ...\n",
      "  [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ...  0.0000000e+00\n",
      "    0.0000000e+00  0.0000000e+00]\n",
      "  [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ...  0.0000000e+00\n",
      "    0.0000000e+00  0.0000000e+00]\n",
      "  [ 0.0000000e+00  0.0000000e+00  0.0000000e+00 ...  0.0000000e+00\n",
      "    0.0000000e+00  0.0000000e+00]]]\n",
      "test_y:\n",
      "[0 0 1 1 2 2 0 0 1 1 2 2 0 0 0 0 1 1 1 1 2 2 2 2 0 0 0 0 1 1 1 1 2 2 2 2 0\n",
      " 0 0 0 1 1 1 1 2 2 2 2]\n"
     ]
    }
   ],
   "source": [
    "# Split the data into train data and test data, to avoid train data mixed into test data spilit it by time series\n",
    "train_df = df_concat[(df_concat[\"window\"] + 1) / df_concat[\"windows_count\"] < 0.8].copy()\n",
    "train_df.drop([\"window\", \"windows_count\"], axis=1, inplace=True)\n",
    "test_df = df_concat[df_concat[\"window\"] / df_concat[\"windows_count\"] >= 0.8].copy()\n",
    "test_df.drop([\"window\", \"windows_count\"], axis=1, inplace=True)\n",
    "print(train_df)\n",
    "print(test_df)\n",
    "\n",
    "# Define a function to divide the data into X and y\n",
    "def divide_Xy(df, oversample=True):\n",
    "    data = df.to_numpy()\n",
    "    X = data[:, :-1]\n",
    "    y = data[:, 1].astype(np.int32)\n",
    "    if oversample:\n",
    "        ros = RandomOverSampler(random_state=0)\n",
    "        X, y = ros.fit_resample(X, y)\n",
    "    X = np.stack([item[0] for item in X])\n",
    "    X = X.astype(np.float32)\n",
    "    return X, y\n",
    "\n",
    "# Divide the train data and test data into X and y\n",
    "train_X, train_y = divide_Xy(train_df)\n",
    "test_X, test_y = divide_Xy(test_df, oversample=False)\n",
    "\n",
    "# Print the data\n",
    "print(\"train_X:\")\n",
    "print(train_X)\n",
    "print(\"train_y:\")\n",
    "print(train_y)\n",
    "print(\"test_X:\")\n",
    "print(test_X)\n",
    "print(\"test_y:\")\n",
    "print(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "326ddeb4",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:cpu\n",
      "Epoch 1 loss: 0.032633356682889095\n",
      "Epoch 2 loss: 0.02258931006397213\n",
      "Epoch 3 loss: 0.01799648221548613\n",
      "Epoch 4 loss: 0.0174991461607787\n",
      "Epoch 5 loss: 0.017405832404488914\n",
      "Epoch 6 loss: 0.01739058730838535\n",
      "Epoch 7 loss: 0.01738940380715035\n",
      "Epoch 8 loss: 0.017388711909990053\n",
      "Epoch 9 loss: 0.01738839616646638\n",
      "Epoch 10 loss: 0.017388268097026927\n",
      "Epoch 11 loss: 0.017388211445765453\n",
      "Epoch 12 loss: 0.017388181643443065\n",
      "Epoch 13 loss: 0.017388160164291795\n",
      "Epoch 14 loss: 0.01738814324946017\n",
      "Epoch 15 loss: 0.017388124455202807\n",
      "Epoch 16 loss: 0.01738810646641362\n",
      "Epoch 17 loss: 0.01738808901460321\n",
      "Epoch 18 loss: 0.017388071831282194\n",
      "Epoch 19 loss: 0.017388055990408133\n",
      "Epoch 20 loss: 0.01738803988104468\n",
      "Finished Training\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        16\n",
      "           1       1.00      1.00      1.00        16\n",
      "           2       1.00      1.00      1.00        16\n",
      "\n",
      "    accuracy                           1.00        48\n",
      "   macro avg       1.00      1.00      1.00        48\n",
      "weighted avg       1.00      1.00      1.00        48\n",
      "\n",
      "Confusion matrix: \n",
      "[[16  0  0]\n",
      " [ 0 16  0]\n",
      " [ 0  0 16]]\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device:{device}\")\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(13, 30, 3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(30, 60, 3, padding=1)\n",
    "        self.pool = nn.MaxPool1d(2, 2)\n",
    "        self.fc1 = nn.Linear(60 * 50, 120)\n",
    "        self.fc2 = nn.Linear(120, 80)\n",
    "        self.fc3 = nn.Linear(80, 3)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.relu = nn.ReLU()\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.relu(self.conv1(x)))\n",
    "        x = self.pool(self.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.softmax(self.fc3(x))\n",
    "        return x\n",
    "    def predict(self, X):\n",
    "        X = torch.tensor(X, dtype=torch.float32, device=device)\n",
    "        with torch.no_grad():\n",
    "            outputs = self(X)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "        return predicted.cpu().numpy()\n",
    "\n",
    "# Train the model with train_X and train_y\n",
    "def train_model(model, train_X, train_y, epochs=10, batch_size=32, lr=0.01):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    model.to(device)\n",
    "    train_X = torch.tensor(train_X, dtype=torch.float32, device=device)\n",
    "    train_y = torch.tensor(train_y, dtype=torch.long, device=device)\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for i in range(0, len(train_X), batch_size):\n",
    "            inputs = train_X[i:i+batch_size]\n",
    "            labels = train_y[i:i+batch_size]\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        print(f\"Epoch {epoch + 1} loss: {running_loss / len(train_X)}\")\n",
    "    print(\"Finished Training\")\n",
    "    return model\n",
    "\n",
    "# Call train_model to train the model\n",
    "cnn_model = CNN()\n",
    "train_X_CNN = train_X.transpose(0, 2, 1)\n",
    "train_y_CNN = train_y\n",
    "test_X_CNN = test_X.transpose(0, 2, 1)\n",
    "test_y_CNN = test_y\n",
    "cnn_model = train_model(cnn_model, train_X_CNN, train_y_CNN, epochs=20, lr=0.001)\n",
    "def evaluate_model(model, X, y):\n",
    "    predicted_y = model.predict(X)\n",
    "    print(classification_report(y, predicted_y))\n",
    "\n",
    "    confusion_mat = confusion_matrix(y, predicted_y)\n",
    "    print(f\"Confusion matrix: \\n{confusion_mat}\")\n",
    "evaluate_model(cnn_model, test_X_CNN, test_y_CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f5a3a7d7",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 loss: 0.030163585334210784\n",
      "Epoch 2 loss: 0.025015343685407896\n",
      "Epoch 3 loss: 0.01918162123577015\n",
      "Epoch 4 loss: 0.018589575816919137\n",
      "Epoch 5 loss: 0.01843693637633109\n",
      "Epoch 6 loss: 0.01882096265887355\n",
      "Epoch 7 loss: 0.018951795122644922\n",
      "Epoch 8 loss: 0.018924194413262443\n",
      "Epoch 9 loss: 0.017992255923984287\n",
      "Epoch 10 loss: 0.01767451537621988\n",
      "Epoch 11 loss: 0.017392839397396054\n",
      "Epoch 12 loss: 0.01739197781494072\n",
      "Epoch 13 loss: 0.017391434392413578\n",
      "Epoch 14 loss: 0.017391063608564773\n",
      "Epoch 15 loss: 0.01739078572204521\n",
      "Epoch 16 loss: 0.017390566366212862\n",
      "Epoch 17 loss: 0.017390384061916453\n",
      "Epoch 18 loss: 0.0173902296805167\n",
      "Epoch 19 loss: 0.017390093556395522\n",
      "Epoch 20 loss: 0.01738997381012719\n",
      "Epoch 21 loss: 0.017389865608902665\n",
      "Epoch 22 loss: 0.01738976922121134\n",
      "Epoch 23 loss: 0.017389679277265393\n",
      "Epoch 24 loss: 0.017389598193469347\n",
      "Epoch 25 loss: 0.01738952247946112\n",
      "Epoch 26 loss: 0.017389453746177053\n",
      "Epoch 27 loss: 0.017389389308723243\n",
      "Epoch 28 loss: 0.017389328630120906\n",
      "Epoch 29 loss: 0.01738927224734882\n",
      "Epoch 30 loss: 0.01738922016040699\n",
      "Finished Training\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        16\n",
      "           1       1.00      1.00      1.00        16\n",
      "           2       1.00      1.00      1.00        16\n",
      "\n",
      "    accuracy                           1.00        48\n",
      "   macro avg       1.00      1.00      1.00        48\n",
      "weighted avg       1.00      1.00      1.00        48\n",
      "\n",
      "Confusion matrix: \n",
      "[[16  0  0]\n",
      " [ 0 16  0]\n",
      " [ 0  0 16]]\n"
     ]
    }
   ],
   "source": [
    "class GRU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GRU, self).__init__()\n",
    "        self.gru = nn.GRU(13, 70, 1, batch_first=True)\n",
    "        self.fc1 = nn.Linear(70, 3)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "    def forward(self, x):\n",
    "        x, _ = self.gru(x)\n",
    "        x = self.fc1(x[:, -1, :])\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "    def predict(self, X):\n",
    "        X = torch.tensor(X, dtype=torch.float32, device=device)\n",
    "        with torch.no_grad():\n",
    "            outputs = self(X)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "        return predicted.cpu().numpy()\n",
    "gru_model = GRU()\n",
    "train_X_GRU = train_X\n",
    "train_y_GRU = train_y\n",
    "test_X_GRU = test_X\n",
    "test_y_GRU = test_y\n",
    "gru_model = train_model(gru_model, train_X_GRU, train_y_GRU, epochs=30)\n",
    "evaluate_model(gru_model, test_X_GRU, test_y_GRU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "458751e5",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 loss: 0.029453183348114427\n",
      "Epoch 2 loss: 0.02141991740948445\n",
      "Epoch 3 loss: 0.018353496048901533\n",
      "Epoch 4 loss: 0.01765933042173987\n",
      "Epoch 5 loss: 0.0174864044597557\n",
      "Epoch 6 loss: 0.017444736248738057\n",
      "Epoch 7 loss: 0.017428314632123656\n",
      "Epoch 8 loss: 0.01742053380957595\n",
      "Epoch 9 loss: 0.017416065340643532\n",
      "Epoch 10 loss: 0.01741309423704405\n",
      "Epoch 11 loss: 0.017410889133676753\n",
      "Epoch 12 loss: 0.017409118983122678\n",
      "Epoch 13 loss: 0.017407623228726087\n",
      "Epoch 14 loss: 0.017406315148413717\n",
      "Epoch 15 loss: 0.017405148025031562\n",
      "Epoch 16 loss: 0.01740409500964053\n",
      "Epoch 17 loss: 0.01740313999287717\n",
      "Epoch 18 loss: 0.017402269013293156\n",
      "Epoch 19 loss: 0.017401472942249196\n",
      "Epoch 20 loss: 0.01740073996621209\n",
      "Epoch 21 loss: 0.01740006203050012\n",
      "Epoch 22 loss: 0.01739942759006947\n",
      "Epoch 23 loss: 0.017398834765494406\n",
      "Epoch 24 loss: 0.017398276844540157\n",
      "Epoch 25 loss: 0.017397761076420278\n",
      "Epoch 26 loss: 0.01739729444185893\n",
      "Epoch 27 loss: 0.017396878014813672\n",
      "Epoch 28 loss: 0.017396495685921057\n",
      "Epoch 29 loss: 0.017396143427840224\n",
      "Epoch 30 loss: 0.01739581506531518\n",
      "Epoch 31 loss: 0.017395505497047492\n",
      "Epoch 32 loss: 0.01739521713944169\n",
      "Epoch 33 loss: 0.017394944891199336\n",
      "Epoch 34 loss: 0.01739468955778861\n",
      "Epoch 35 loss: 0.017394447648847424\n",
      "Epoch 36 loss: 0.017394219432865177\n",
      "Epoch 37 loss: 0.01739400329890552\n",
      "Epoch 38 loss: 0.017393800589415403\n",
      "Epoch 39 loss: 0.017393606471585797\n",
      "Epoch 40 loss: 0.017393422287863655\n",
      "Epoch 41 loss: 0.0173932472327808\n",
      "Epoch 42 loss: 0.017393083454252365\n",
      "Epoch 43 loss: 0.017392925582490525\n",
      "Epoch 44 loss: 0.01739277442296346\n",
      "Epoch 45 loss: 0.0173926318550969\n",
      "Epoch 46 loss: 0.01739249626795451\n",
      "Epoch 47 loss: 0.01739236551362115\n",
      "Epoch 48 loss: 0.017392241203033173\n",
      "Epoch 49 loss: 0.017392122262233013\n",
      "Epoch 50 loss: 0.017392008422731278\n",
      "Finished Training\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        16\n",
      "           1       1.00      1.00      1.00        16\n",
      "           2       1.00      1.00      1.00        16\n",
      "\n",
      "    accuracy                           1.00        48\n",
      "   macro avg       1.00      1.00      1.00        48\n",
      "weighted avg       1.00      1.00      1.00        48\n",
      "\n",
      "Confusion matrix: \n",
      "[[16  0  0]\n",
      " [ 0 16  0]\n",
      " [ 0  0 16]]\n"
     ]
    }
   ],
   "source": [
    "class CNN_GRU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN_GRU, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(13, 30, 3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(30, 60, 3, padding=1)\n",
    "        self.pool = nn.MaxPool1d(2, 2)\n",
    "        self.gru = nn.GRU(60, 20, 1, batch_first=True)\n",
    "        self.fc1 = nn.Linear(20, 3)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.relu(self.conv1(x)))\n",
    "        x = self.pool(self.relu(self.conv2(x)))\n",
    "        x = x.transpose(2, 1)\n",
    "        x, _ = self.gru(x)\n",
    "        x = self.fc1(x[:, -1, :])\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "    def predict(self, X):\n",
    "        X = torch.tensor(X, dtype=torch.float32, device=device)\n",
    "        with torch.no_grad():\n",
    "            outputs = self(X)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "        return predicted.cpu().numpy()\n",
    "\n",
    "cnn_gru_model = CNN_GRU()\n",
    "train_X_CNN_GRU = train_X_CNN\n",
    "train_y_CNN_GRU = train_y_CNN\n",
    "test_X_CNN_GRU = test_X_CNN\n",
    "test_y_CNN_GRU = test_y_CNN\n",
    "cnn_gru_model = train_model(cnn_gru_model, train_X_CNN_GRU, train_y_CNN_GRU, epochs=50)\n",
    "evaluate_model(cnn_gru_model, test_X_CNN_GRU, test_y_CNN_GRU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41527487",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
