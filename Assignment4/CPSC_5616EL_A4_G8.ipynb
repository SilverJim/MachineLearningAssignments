{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d70e062",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the required machine learning libraries and models\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score, classification_report, confusion_matrix\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d482fc1e",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# All the data should put into the data folder\n",
    "data_filename = 'data'\n",
    "\n",
    "if sys.modules.get(\"google.colab\") is None:\n",
    "    data_path_prefix = \".\"\n",
    "else:\n",
    "    from google.colab import drive\n",
    "    drive.mount(\"/content/drive\")\n",
    "    data_path_prefix = \"/content/drive/MyDrive/MachineLearningAssignments/Assignment4\"\n",
    "\n",
    "data_path = f\"{data_path_prefix}/{data_filename}\"\n",
    "\n",
    "print(f\"Loading data from data path: {data_path}\")\n",
    "\n",
    "# Define the exercise types\n",
    "data_sets = [\n",
    "   {\"path\": \"Jumping_Jack_x10\", \"name\": \"Jumping_Jack\"}, \n",
    "   {\"path\": \"Lunges_x10\", \"name\": \"Lunges\"}, \n",
    "   {\"path\": \"Squat_x10\", \"name\": \"Squat\"}, \n",
    "   {\"path\": \"Jumping_Jack_x10_new\", \"name\": \"Jumping_Jack_new\"}, \n",
    "   {\"path\": \"Lunges_x10_new\", \"name\": \"Lunges_new\"}, \n",
    "   {\"path\": \"Squat_x10\", \"name\": \"Squat_new\"}, \n",
    "]\n",
    "\n",
    "# Define the data sets\n",
    "data_set = [(\"Accelerometer.csv\", \"Accelerometer\"), (\"TotalAcceleration.csv\", \"TotalAcceleration\"), (\"Orientation.csv\", \"Orientation\")]\n",
    "\n",
    "df_list = []\n",
    "\n",
    "# In all the data set after \"time\" is removed, only \"seconds_elapsed\" and \"data_set\" are non-value columns\n",
    "non_value_columns = [\"seconds_elapsed\", \"data_set\"]\n",
    "non_value_columns_set = set(non_value_columns)\n",
    "# read every dataset in every exercise type\n",
    "for file_name, data_set_name in data_set:\n",
    "    temp_df_list = []\n",
    "    for data_set in data_sets:\n",
    "        df = pd.read_csv(f\"{data_path}/{data_set['path']}/{file_name}\")\n",
    "        # There is \"seconds_elapsed\" so \"time\" is not necessary\n",
    "        df.drop(\"time\", axis=1, inplace=True)\n",
    "        # add the data_set column according to which exercise the data from\n",
    "        df['data_set'] = data_set['name']\n",
    "        # Rename the columns according to which dataset the data from to avoid same column\n",
    "        for column in df.columns:\n",
    "            if column not in non_value_columns_set:\n",
    "                df.rename(columns={column: f\"{data_set_name}_{column}\"}, inplace=True)\n",
    "        value_columns = list(set(df.columns) - non_value_columns_set)\n",
    "        # Noise Reduction: calculate rolling mean for all the value columns\n",
    "        df[value_columns] = df[value_columns].rolling(50).mean()\n",
    "        # rolling mean create some NaN values, so we should drop them\n",
    "        df.dropna(inplace=True)\n",
    "        # remove the data at beginning and end of each exercise, as the excercise has not started or has ended\n",
    "        df = df[(df[\"seconds_elapsed\"].quantile(0.1) < df[\"seconds_elapsed\"]) & (df['seconds_elapsed'] < df['seconds_elapsed'].quantile(0.9))]\n",
    "        temp_df_list.append(df)\n",
    "    # combine dataframe from all the exercise types together and put it in to the df_list\n",
    "    df_list.append(pd.concat(temp_df_list))\n",
    "    \n",
    "# Normalization\n",
    "for i in range(len(df_list)):\n",
    "    df = df_list[i]\n",
    "    value_columns = list(set(df.columns) - non_value_columns_set)\n",
    "    scaler = ColumnTransformer(\n",
    "        [\n",
    "            ('standard_scaler', StandardScaler(), value_columns),\n",
    "            ('other', 'passthrough', non_value_columns)\n",
    "\n",
    "        ],\n",
    "    )\n",
    "    scaled_data = scaler.fit_transform(df)\n",
    "    df_list[i] = pd.DataFrame(scaled_data, columns=value_columns + non_value_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb9dbd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Window by dividing the data of each exercise into eaqual parts to apply data augmentation we use several different window sizes\n",
    "\n",
    "# If the length of the array is less than the target length, pad it with zeros and make the original array at the beginning\n",
    "def pad(original_series, target_length):\n",
    "    new_series = original_series.copy()\n",
    "    for i in range(original_series.size):\n",
    "        line = original_series[i]\n",
    "        original_length = len(line)\n",
    "        if original_length > target_length:\n",
    "            new_line = line[:target_length]\n",
    "        else:\n",
    "            new_line = np.pad(line, ((0, target_length - original_length), (0, 0)), mode=\"constant\")\n",
    "        new_series[i] = new_line\n",
    "    return new_series\n",
    "\n",
    "windows_count_list = [8, 9, 10, 11, 12]\n",
    "df_concat_list = []\n",
    "for windows_count in windows_count_list:\n",
    "    seconds_elapsed_min = {}\n",
    "    windows_size = {}\n",
    "    for item in data_sets:\n",
    "        data_set = item[\"name\"]\n",
    "        seconds_elapsed_min[data_set] = min((df[df[\"data_set\"] == data_set][\"seconds_elapsed\"].min() for df in df_list))\n",
    "        seconds_elapsed_max = max((df[df[\"data_set\"] == data_set][\"seconds_elapsed\"].max() for df in df_list)) + 0.0000001\n",
    "        windows_size[data_set] = (seconds_elapsed_max - seconds_elapsed_min[data_set]) / windows_count\n",
    "    print(seconds_elapsed_min, windows_size)\n",
    "\n",
    "    # process window and calculate min, max, mean, std value of each window\n",
    "    count_after_padding = 200\n",
    "    df_list_temp = []\n",
    "    for i, df in enumerate(df_list):\n",
    "        df = df.copy()\n",
    "        df[\"window\"] = df[[\"seconds_elapsed\", \"data_set\"]].apply(lambda row: math.floor((row[\"seconds_elapsed\"] - seconds_elapsed_min[row[\"data_set\"]]) / windows_size[row[\"data_set\"]]), axis=1)\n",
    "        df = df.sort_values([\"data_set\", \"window\", \"seconds_elapsed\"])\n",
    "        df.drop(\"seconds_elapsed\", axis=1, inplace=True)\n",
    "        df = (\n",
    "            df\n",
    "            .groupby([\"data_set\", \"window\"], group_keys=True)\n",
    "            .apply(lambda x: np.array(x[[column for column in df.columns if column not in {\"data_set\", \"window\"}]]))\n",
    "        ).to_frame()\n",
    "        df[0] = df.apply(lambda x: pad(x, count_after_padding))\n",
    "        df.rename(columns={0: i}, inplace=True)\n",
    "        df_list_temp.append(df)\n",
    "    # join the dataframe from all the dataset together according to index \"windows\" and \"data_set\"\n",
    "    df_concat_temp = pd.concat(df_list_temp, axis=1)\n",
    "    df_concat_temp[\"windows_count\"] = windows_count\n",
    "    df_concat_list.append(df_concat_temp)\n",
    "df_concat = pd.concat(df_concat_list)\n",
    "df_concat.reset_index(inplace=True)\n",
    "# Make the data_set the last column\n",
    "data_set_mappping = {\n",
    "    \"Jumping_Jack\": 0,\n",
    "    \"Lunges\": 1,\n",
    "    \"Squat\": 2,\n",
    "    \"Jumping_Jack_new\": 0,\n",
    "    \"Lunges_new\": 1,\n",
    "    \"Squat_new\": 2,\n",
    "}\n",
    "df_concat[\"exercise_type\"] = df_concat[\"data_set\"].map(data_set_mappping)\n",
    "df_concat.drop([\"data_set\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a09766f",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "df_concat[\"features\"] = df_concat.apply(lambda x:np.concatenate([x[0], x[1], x[2]], axis=1), axis=1)\n",
    "df_concat.drop([0, 1, 2], axis=1, inplace=True)\n",
    "temp = df_concat.pop(\"exercise_type\")\n",
    "df_concat[\"exercise_type\"] = temp\n",
    "print(df_concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d010f907",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# Split the data into train data and test data, to avoid train data mixed into test data spilit it by time series\n",
    "train_df = df_concat[(df_concat[\"window\"] + 1) / df_concat[\"windows_count\"] < 0.8].copy()\n",
    "train_df.drop([\"window\", \"windows_count\"], axis=1, inplace=True)\n",
    "test_df = df_concat[df_concat[\"window\"] / df_concat[\"windows_count\"] >= 0.8].copy()\n",
    "test_df.drop([\"window\", \"windows_count\"], axis=1, inplace=True)\n",
    "print(train_df)\n",
    "print(test_df)\n",
    "\n",
    "# Define a function to divide the data into X and y\n",
    "def divide_Xy(df, oversample=True):\n",
    "    data = df.to_numpy()\n",
    "    X = data[:, :-1]\n",
    "    y = data[:, 1].astype(np.int32)\n",
    "    if oversample:\n",
    "        ros = RandomOverSampler(random_state=0)\n",
    "        X, y = ros.fit_resample(X, y)\n",
    "    X = np.stack([item[0] for item in X])\n",
    "    X = X.astype(np.float32)\n",
    "    return X, y\n",
    "\n",
    "# Divide the train data and test data into X and y\n",
    "train_X, train_y = divide_Xy(train_df)\n",
    "test_X, test_y = divide_Xy(test_df, oversample=False)\n",
    "\n",
    "# Print the data\n",
    "print(\"train_X:\")\n",
    "print(train_X)\n",
    "print(\"train_y:\")\n",
    "print(train_y)\n",
    "print(\"test_X:\")\n",
    "print(test_X)\n",
    "print(\"test_y:\")\n",
    "print(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845eeae9",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device:{device}\")\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(13, 30, 3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(30, 60, 3, padding=1)\n",
    "        self.pool = nn.MaxPool1d(2, 2)\n",
    "        self.fc1 = nn.Linear(60 * 50, 120)\n",
    "        self.fc2 = nn.Linear(120, 80)\n",
    "        self.fc3 = nn.Linear(80, 3)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.relu = nn.ReLU()\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.relu(self.conv1(x)))\n",
    "        x = self.pool(self.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.softmax(self.fc3(x))\n",
    "        return x\n",
    "    def predict(self, X):\n",
    "        X = torch.tensor(X, dtype=torch.float32, device=device)\n",
    "        with torch.no_grad():\n",
    "            outputs = self(X)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "        return predicted.cpu().numpy()\n",
    "\n",
    "# Train the model with train_X and train_y\n",
    "def train_model(model, train_X, train_y, epochs=10, batch_size=32, lr=0.01):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    model.to(device)\n",
    "    train_X = torch.tensor(train_X, dtype=torch.float32, device=device)\n",
    "    train_y = torch.tensor(train_y, dtype=torch.long, device=device)\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for i in range(0, len(train_X), batch_size):\n",
    "            inputs = train_X[i:i+batch_size]\n",
    "            labels = train_y[i:i+batch_size]\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        print(f\"Epoch {epoch + 1} loss: {running_loss / len(train_X)}\")\n",
    "    print(\"Finished Training\")\n",
    "    return model\n",
    "\n",
    "# Call train_model to train the model\n",
    "cnn_model = CNN()\n",
    "train_X_CNN = train_X.transpose(0, 2, 1)\n",
    "train_y_CNN = train_y\n",
    "test_X_CNN = test_X.transpose(0, 2, 1)\n",
    "test_y_CNN = test_y\n",
    "cnn_model = train_model(cnn_model, train_X_CNN, train_y_CNN)\n",
    "def evaluate_model(model, X, y):\n",
    "    predicted_y = model.predict(X)\n",
    "    print(classification_report(y, predicted_y))\n",
    "\n",
    "    confusion_mat = confusion_matrix(y, predicted_y)\n",
    "    print(f\"Confusion matrix: \\n{confusion_mat}\")\n",
    "evaluate_model(cnn_model, test_X_CNN, test_y_CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ba97cc",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "class GRU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GRU, self).__init__()\n",
    "        self.gru = nn.GRU(13, 50, 1, batch_first=True)\n",
    "        self.fc1 = nn.Linear(50, 3)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "    def forward(self, x):\n",
    "        x, _ = self.gru(x)\n",
    "        x = self.fc1(x[:, -1, :])\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "    def predict(self, X):\n",
    "        X = torch.tensor(X, dtype=torch.float32, device=device)\n",
    "        with torch.no_grad():\n",
    "            outputs = self(X)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "        return predicted.cpu().numpy()\n",
    "gru_model = GRU()\n",
    "train_X_GRU = train_X\n",
    "train_y_GRU = train_y\n",
    "test_X_GRU = test_X\n",
    "test_y_GRU = test_y\n",
    "gru_model = train_model(gru_model, train_X_GRU, train_y_GRU, epochs=60)\n",
    "evaluate_model(gru_model, test_X_GRU, test_y_GRU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2550341",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "class CNN_GRU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN_GRU, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(13, 30, 3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(30, 60, 3, padding=1)\n",
    "        self.pool = nn.MaxPool1d(2, 2)\n",
    "        self.gru = nn.GRU(60, 20, 1, batch_first=True)\n",
    "        self.fc1 = nn.Linear(20, 3)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.relu(self.conv1(x)))\n",
    "        x = self.pool(self.relu(self.conv2(x)))\n",
    "        x = x.transpose(2, 1)\n",
    "        x, _ = self.gru(x)\n",
    "        x = self.fc1(x[:, -1, :])\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "    def predict(self, X):\n",
    "        X = torch.tensor(X, dtype=torch.float32, device=device)\n",
    "        with torch.no_grad():\n",
    "            outputs = self(X)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "        return predicted.cpu().numpy()\n",
    "\n",
    "cnn_gru_model = CNN_GRU()\n",
    "train_X_CNN_GRU = train_X_CNN\n",
    "train_y_CNN_GRU = train_y_CNN\n",
    "test_X_CNN_GRU = test_X_CNN\n",
    "test_y_CNN_GRU = test_y_CNN\n",
    "cnn_gru_model = train_model(cnn_gru_model, train_X_CNN_GRU, train_y_CNN_GRU, epochs=50)\n",
    "evaluate_model(cnn_gru_model, test_X_CNN_GRU, test_y_CNN_GRU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602900cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
