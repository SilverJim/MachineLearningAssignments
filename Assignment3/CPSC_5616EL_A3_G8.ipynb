{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "# import the required machine learning libraries and models\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score, classification_report, confusion_matrix\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "# All the data should put into the data folder\n",
    "data_filename = 'data'\n",
    "\n",
    "if sys.modules.get(\"google.colab\") is None:\n",
    "    data_path_prefix = \".\"\n",
    "else:\n",
    "    from google.colab import drive\n",
    "    drive.mount(\"/content/drive\")\n",
    "    data_path_prefix = \"/content/drive/MyDrive/MachineLearningAssignments/Assignment3\"\n",
    "\n",
    "data_path = f\"{data_path_prefix}/{data_filename}\"\n",
    "\n",
    "print(f\"Loading data from data path: {data_path}\")\n",
    "\n",
    "# Define the exercise types\n",
    "exercise_types = [\n",
    "   {\"path\": \"Jumping_Jack_x10\", \"name\": \"Jumping_Jack\"}, \n",
    "   {\"path\": \"Lunges_x10\", \"name\": \"Lunges\"}, \n",
    "   {\"path\": \"Squat_x10\", \"name\": \"Squat\"}, \n",
    "]\n",
    "\n",
    "# Define the data sets\n",
    "data_set = [(\"Accelerometer.csv\", \"Accelerometer\"), (\"TotalAcceleration.csv\", \"TotalAcceleration\"), (\"Orientation.csv\", \"Orientation\")]\n",
    "\n",
    "df_list = []\n",
    "\n",
    "# In all the data set after \"time\" is removed, only \"seconds_elapsed\" and \"exercise_type\" are non-value columns\n",
    "non_value_columns = [\"seconds_elapsed\", \"exercise_type\"]\n",
    "non_value_columns_set = set(non_value_columns)\n",
    "# read every dataset in every exercise type\n",
    "for file_name, data_set_name in data_set:\n",
    "    temp_df_list = []\n",
    "    for exercise_type in exercise_types:\n",
    "        df = pd.read_csv(f\"{data_path}/{exercise_type['path']}/{file_name}\")\n",
    "        # There is \"seconds_elapsed\" so \"time\" is not necessary\n",
    "        df.drop(\"time\", axis=1, inplace=True)\n",
    "        # add the exercise_type column according to which exercise the data from\n",
    "        df['exercise_type'] = exercise_type['name']\n",
    "        # Rename the columns according to which dataset the data from to avoid same column\n",
    "        for column in df.columns:\n",
    "            if column not in non_value_columns_set:\n",
    "                df.rename(columns={column: f\"{data_set_name}_{column}\"}, inplace=True)\n",
    "        value_columns = list(set(df.columns) - non_value_columns_set)\n",
    "        # Noise Reduction: calculate rolling mean for all the value columns\n",
    "        df[value_columns] = df[value_columns].rolling(50).mean()\n",
    "        # rolling mean create some NaN values, so we should drop them\n",
    "        df.dropna(inplace=True)\n",
    "        # remove the data at beginning and end of each exercise, as the excercise has not started or has ended\n",
    "        df = df[(df[\"seconds_elapsed\"].quantile(0.1) < df[\"seconds_elapsed\"]) & (df['seconds_elapsed'] < df['seconds_elapsed'].quantile(0.9))]\n",
    "        temp_df_list.append(df)\n",
    "    # combine dataframe from all the exercise types together and put it in to the df_list\n",
    "    df_list.append(pd.concat(temp_df_list))\n",
    "    \n",
    "# Normalization\n",
    "for i in range(len(df_list)):\n",
    "    df = df_list[i]\n",
    "    value_columns = list(set(df.columns) - non_value_columns_set)\n",
    "    scaler = ColumnTransformer(\n",
    "        [\n",
    "            ('standard_scaler', StandardScaler(), value_columns),\n",
    "            ('other', 'passthrough', non_value_columns)\n",
    "\n",
    "        ],\n",
    "    )\n",
    "    scaled_data = scaler.fit_transform(df)\n",
    "    df_list[i] = pd.DataFrame(scaled_data, columns=value_columns + non_value_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Window by dividing the data of each exercise into eaqual parts to apply data augmentation we use several different window sizes\n",
    "windows_count_list = [8, 9, 10, 11, 12]\n",
    "df_concat_list = []\n",
    "for windows_count in windows_count_list:\n",
    "    seconds_elapsed_min = {}\n",
    "    windows_size = {}\n",
    "    for item in exercise_types:\n",
    "        exercise_type = item[\"name\"]\n",
    "        seconds_elapsed_min[exercise_type] = min((df[df[\"exercise_type\"] == exercise_type][\"seconds_elapsed\"].min() for df in df_list))\n",
    "        seconds_elapsed_max = max((df[df[\"exercise_type\"] == exercise_type][\"seconds_elapsed\"].max() for df in df_list)) + 0.0000001\n",
    "        windows_size[exercise_type] = (seconds_elapsed_max - seconds_elapsed_min[exercise_type]) / windows_count\n",
    "    print(seconds_elapsed_min, windows_size)\n",
    "\n",
    "    # process window and calculate min, max, mean, std value of each window\n",
    "    df_list_temp = []\n",
    "    for df in df_list:\n",
    "        df = df.copy()\n",
    "        df[\"window\"] = df[[\"seconds_elapsed\", \"exercise_type\"]].apply(lambda row: math.floor((row[\"seconds_elapsed\"] - seconds_elapsed_min[row[\"exercise_type\"]]) / windows_size[row[\"exercise_type\"]]), axis=1)\n",
    "        df.drop(\"seconds_elapsed\", axis=1, inplace=True)\n",
    "        df = df.groupby([\"exercise_type\", \"window\"]).agg({column: [\"min\",\"max\",\"mean\",\"std\"] for column in df.columns.difference([\"exercise_type\", \"window\"])})\n",
    "        df_list_temp.append(df)\n",
    "    # join the dataframe from all the dataset together according to index \"windows\" and \"exercise_type\"\n",
    "    df_concat_temp = pd.concat(df_list_temp, axis=1)\n",
    "    df_concat_temp[\"windows_count\"] = windows_count\n",
    "    df_concat_list.append(df_concat_temp)\n",
    "df_concat = pd.concat(df_concat_list)\n",
    "df_concat.reset_index(inplace=True)\n",
    "# Make the exercise_type the last column\n",
    "temp = df_concat.pop(\"exercise_type\")\n",
    "df_concat[\"exercise_type\"] = temp\n",
    "print(df_concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Split the data into train data and test data, to avoid train data mixed into test data spilit it by time series\n",
    "train_df = df_concat[(df_concat[\"window\"] + 1) / df_concat[\"windows_count\"] < 0.8].copy()\n",
    "train_df.drop([\"window\", \"windows_count\"], axis=1, level=0, inplace=True)\n",
    "test_df = df_concat[df_concat[\"window\"] / df_concat[\"windows_count\"] >= 0.8].copy()\n",
    "test_df.drop([\"window\", \"windows_count\"], axis=1, level=0, inplace=True)\n",
    "print(train_df)\n",
    "print(test_df)\n",
    "\n",
    "# Define a function to divide the data into X and y\n",
    "def divide_Xy(df, oversample=True):\n",
    "    data = df.to_numpy()\n",
    "    X = data[:, :-1]\n",
    "    y = data[:, -1]\n",
    "    if oversample:\n",
    "        ros = RandomOverSampler(random_state=0)\n",
    "        X, y = ros.fit_resample(X, y)\n",
    "    return data, X, y\n",
    "\n",
    "# Divide the train data and test data into X and y\n",
    "train_data, train_X, train_y = divide_Xy(train_df)\n",
    "test_data, test_X, test_y = divide_Xy(test_df, oversample=False)\n",
    "\n",
    "# Print the data\n",
    "print(\"train_X:\")\n",
    "print(train_X)\n",
    "print(\"train_y:\")\n",
    "print(train_y)\n",
    "print(\"test_X:\")\n",
    "print(test_X)\n",
    "print(\"test_y:\")\n",
    "print(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# Using GridSearchCV to find the SVC model with best parameters\n",
    "print(\"Finding best parameters for SVM model:\")\n",
    "svm_model = SVC()\n",
    "svm_grid_search = GridSearchCV(svm_model, {\"kernel\": [\"linear\", \"poly\", \"rbf\", \"sigmoid\"], 'C': [0.1, 1, 10]}, cv=5)\n",
    "svm_grid_search.fit(train_X, train_y)\n",
    "print(f\"Best parameters for SVM model:\\n{svm_grid_search.best_params_}\")\n",
    "svm_model = svm_grid_search.best_estimator_\n",
    "\n",
    "# Defind a function to evaluate the models\n",
    "def evaluate_model(model, X, y):\n",
    "    predicted_y = model.predict(X)\n",
    "    print(classification_report(y, predicted_y))\n",
    "\n",
    "    confusion_mat = confusion_matrix(y, predicted_y)\n",
    "    print(f\"Confusion matrix: \\n{confusion_mat}\")\n",
    "\n",
    "# Evaluate the the SVC model\n",
    "print(\"Evaluation of SVM model:\")\n",
    "evaluate_model(svm_model, test_X, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %%\n",
    "# Using GridSearchCV to find the RandomForestClassifier model with best parameters\n",
    "print(\"Finding best parameters for Random Forest model:\")\n",
    "rf_model = RandomForestClassifier()\n",
    "rf_grid_search = GridSearchCV(rf_model, {\"n_estimators\": [10, 100, 1000], \"max_depth\": [2, 5, 10]}, cv=5)\n",
    "rf_grid_search.fit(train_X, train_y)\n",
    "print(f\"Best parameters for Random Forest model:\\n{rf_grid_search.best_params_}\")\n",
    "rf_model = rf_grid_search.best_estimator_\n",
    "\n",
    "# Evaluate the the RandomForestClassifier model\n",
    "print(\"Evaluation of Random Forest model:\")\n",
    "evaluate_model(rf_model, test_X, test_y)\n",
    "# %%\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
